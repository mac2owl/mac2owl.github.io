{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>A bunch of notes on Python, Javascript, SQL etc</p>"},{"location":"#useful-linkswebsites","title":"Useful links/websites","text":""},{"location":"#dev-environment","title":"Dev environment","text":"<p>poetry, pyenv, pre-commmit, VS code etc setup - Python Best Practices for a New Project</p> <p>FastAPI template</p>"},{"location":"#text-extraction-librariesservices","title":"Text extraction libraries/services","text":"<p>Docling</p> <p>PyMuPDF4</p> <p>PyMuPDF4LLM</p> <p>Adobe PDF Extract API</p>"},{"location":"SQL/","title":"SQL","text":""},{"location":"SQL/#postgres","title":"Postgres","text":""},{"location":"SQL/#tools-and-cheatsheet","title":"Tools and cheatsheet","text":"<p>Online playground: DB Fiddle GUI tool: TablePlus Postgres.app (Mac) Cheatsheet PostgreSQL Wiki</p>"},{"location":"SQL/#enable-uuid-generation","title":"Enable uuid generation","text":"<pre><code>CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n</code></pre>"},{"location":"SQL/#create-table-with-foreign-key-constraintsreferences","title":"Create table with foreign key constraints/references","text":"<pre><code>CREATE TABLE departments (\n    id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\n    name VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\n\nCREATE TABLE employees (\n    id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\n    email VARCHAR(255) UNIQUE NOT NULL,\n    first_name VARCHAR(255) NOT NULL,\n    last_name VARCHAR(255) NOT NULL,\n    department_id uuid NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL,\n    FOREIGN KEY (department_id) REFERENCES departments(id) ON DELETE CASCADE\n);\n</code></pre> <p>or</p> <pre><code>CREATE TABLE departments (\n    id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\n    name VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\n\nCREATE TABLE employees (\n    id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\n    email VARCHAR(255) UNIQUE NOT NULL,\n    first_name VARCHAR(255) NOT NULL,\n    last_name VARCHAR(255) NOT NULL,\n    department_id uuid NOT NULL REFERENCES departments(id) ON DELETE CASCADE,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\n</code></pre>"},{"location":"SQL/#insert-values-to-table","title":"Insert values to table","text":"<pre><code>INSERT INTO\n    departments(name)\nVALUES\n    ('HR'),\n    ('Finance');\n\nINSERT INTO\n    employees(email, first_name, last_name, department_id)\nVALUES\n    ('mary@test_co.com', 'Mary', 'Smith', (SELECT id from departments WHERE name='Finance')),\n    ('dave@test_co.com', 'Dave', 'Cole', (SELECT id from departments WHERE name='Finance')),\n    ('jane@test_co.com', 'Jane', 'Hills', (SELECT id from departments WHERE name='Finance')),\n    ('john@test_co.com', 'John', 'Doe', (SELECT id from departments WHERE name='HR'));\n</code></pre>"},{"location":"SQL/#simple-audit-table","title":"Simple audit table","text":"<pre><code>CREATE OR REPLACE FUNCTION employees_audit_func()\n    RETURNS TRIGGER\n    AS $employees_audit$\nBEGIN\n    if (TG_OP = 'UPDATE') THEN\n        INSERT INTO employees_audit\n        SELECT\n            uuid_generate_v4(),\n            'UPDATE',\n            now(),\n            NEW.*;\n    elsif (TG_OP = 'INSERT') THEN\n        INSERT INTO employees_audit\n        SELECT\n            uuid_generate_v4(),\n            'INSERT',\n            now(),\n            NEW.*;\n    elsif (TG_OP = 'DELETE') THEN\n        INSERT INTO employees_audit\n        SELECT\n            uuid_generate_v4(),\n            'DELETE',\n            now(),\n            OLD.*;\n    END IF;\n    RETURN NULL;\nEND;\n$employees_audit$\nLANGUAGE plpgsql;\n\nCREATE TRIGGER employees_audit_trigger\n    AFTER INSERT OR UPDATE OR DELETE ON employees FOR EACH ROW\n    EXECUTE PROCEDURE employees_audit_func();\n</code></pre> <p>or only tracking certain columns on update (e.g all columns except created_at and updated_at)</p> <pre><code>CREATE TRIGGER employees_audit_update_selective_trigger\n    AFTER UPDATE ON employees FOR EACH ROW\n    WHEN ( (to_jsonb(OLD.*) - 'updated_at' - 'created_at') IS DISTINCT FROM  (to_jsonb(NEW.*) - 'updated_at' - 'created_at') )\n    EXECUTE PROCEDURE employees_audit_func();\n</code></pre>"},{"location":"SQL/#updated_at-timestamp-trigger","title":"updated_at timestamp trigger","text":"<pre><code>CREATE OR REPLACE FUNCTION trigger_set_timestamp()\nRETURNS TRIGGER AS $$\nBEGIN\n  NEW.updated_at = NOW();\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER set_timestamp_employees\nBEFORE UPDATE ON employees\nFOR EACH ROW\nEXECUTE PROCEDURE trigger_set_timestamp();\n</code></pre>"},{"location":"SQL/#add-column-and-foreign-key-constraint-to-existing-table","title":"Add column and foreign key constraint to existing table","text":"<pre><code>ALTER TABLE employees\nADD COLUMN department_id uuid DEFAULT NULL, -- Set to NOT NULL once data is populated\nADD CONSTRAINT employee_id_department_id FOREIGN KEY (department_id) REFERENCES departments(id);\n</code></pre> <p>Set column to <code>NOT NULL</code></p> <pre><code>ALTER TABLE employees ALTER COLUMN department_id SET NOT NULL;\n</code></pre>"},{"location":"SQL/#alter-table-column-type-with-casting","title":"Alter table column type (with casting)","text":"<pre><code>CREATE TABLE employees (\n    ...\n    employment_start_year VARCHAR(4) NOT NULL\n    ...\n);\n\nALTER TABLE employees ALTER employment_start_year TYPE INT\nUSING employment_start_year::INTEGER;\n</code></pre>"},{"location":"SQL/#update-values-with-unnest","title":"Update values with unnest","text":"<pre><code>UPDATE\n    employees\nSET\n    email = data_table.email\nFROM (\n    SELECT\n        unnest(ARRAY ['Mary', 'John', 'Dawn']) AS first_name,\n        unnest(ARRAY ['Smith', 'Doe', 'Carter']) AS last_name) AS data_table\nWHERE\n    employees.email = data_table.email;\n</code></pre>"},{"location":"SQL/#json-aggregation-and-functions","title":"JSON aggregation and functions","text":"<pre><code>SELECT\n    dept.name AS department,\n    jsonb_agg(\n        DISTINCT jsonb_build_object (  -- DISTINCT to remove dupes\n            'employee_id', e.id,\n            'email', e.email,\n            'first_name', e.first_name,\n            'last_name', e.last_name\n        )\n    ) AS department_staff\nFROM\n    employees e\n    JOIN departments dept ON e.department_id = dept.id\nGROUP BY\n    d.name, d.id;\n</code></pre> <p>Returns:</p> department department_staff Finance [{\"email\": \"dave@test_co.com\", \"last_name\": \"Cole\", \"first_name\": \"Dave\", \"employee_id\": \"b26339e1-af22-4752-852e-cb51f342bb10\"}, {\"email\": \"jane@test_co.com\", \"last_name\": \"Hills\", \"first_name\": \"Jane\", \"employee_id\": \"710b5de9-f248-49d8-9846-57b31ed143b2\"}, {\"email\": \"mary@test_co.com\", \"last_name\": \"Smith\", \"first_name\": \"Mary\", \"employee_id\": \"e2044159-0576-4a3a-9fe5-9af24bf66102\"}] HR [{\"email\": \"john@test_co.com\", \"last_name\": \"Doe\", \"first_name\": \"John\", \"employee_id\": \"bcdadb45-970e-4d87-8b8a-fab1ccfeddd4\"}] <pre><code>SELECT\n    d.name,\n    jsonb_object_agg(e.email, (concat_ws(' ', e.first_name, e.last_name))) AS department_staff\nFROM\n    employees e\n    JOIN departments d ON e.department_id = d.id\nGROUP BY\n    d.name,\n    d.id;\n</code></pre> <p>Returns:</p> department department_staff Finance {\"dave@test_co.com\": \"Dave Cole\", \"jane@test_co.com\": \"Jane Hills\", \"mary@test_co.com\": \"Mary Smith\"} HR {\"john@test_co.com\": \"John Doe\"}"},{"location":"SQL/#get-week-number-of-a-date","title":"Get week number of a date","text":"<p>N.B. With last day of week is Saturady i.e. new week begins on Sunday</p> <pre><code>CREATE OR REPLACE FUNCTION get_week_number_for_date (date_input date) RETURNS int LANGUAGE plpgsql AS $$ BEGIN RETURN (\n        (\n            $1 - DATE_TRUNC('year', $1)::date\n        ) + DATE_PART('isodow', DATE_TRUNC('year', $1))\n    )::int / 7 + CASE\n        WHEN DATE_PART('isodow', DATE_TRUNC('year', $1)) = 7 THEN 0\n        ELSE 1\n    END;\nEND;\n$$;\n\nget_week_number_for_date('2020-01-01');\n</code></pre>"},{"location":"SQL/#count-business-days-between-2-dates","title":"Count business days between 2 dates","text":"<pre><code>CREATE OR REPLACE FUNCTION business_days_count (from_date date, to_date date)\n    RETURNS int\n    LANGUAGE plpgsql\n    AS $$\nBEGIN\n    RETURN (SELECT\n        count(d::date) AS d\n    FROM\n        generate_series(from_date, to_date, '1 day'::interval) d\nWHERE\n    extract('dow' FROM d)\n    NOT in(0, 6));\nEND;\n$$;\n</code></pre>"},{"location":"SQL/#generate_series","title":"GENERATE_SERIES","text":"<p><code>GENERATE_SERIES</code> is pretty handy when creating time-series dataset</p> <pre><code>SELECT * FROM GENERATE_SERIES(2019, 2021, 1) AS \"year\", GENERATE_SERIES(1, 12, 1) AS \"month\";\n</code></pre> <p>Will generate a year-month table</p> year month 2019 1 2019 2 ... ... 2019 12 2020 1 2020 2 ... ... 2020 12 2021 1 2021 2 ... ... 2021 12 <p>...and can also produce a range of dates/time</p> <pre><code>SELECT * FROM generate_series('2022-01-01','2022-01-02', INTERVAL '1 hour');\n</code></pre> generate_series 2022-01-01T00:00:00.000Z 2022-01-01T01:00:00.000Z 2022-01-01T02:00:00.000Z 2022-01-01T03:00:00.000Z 2022-01-01T04:00:00.000Z 2022-01-01T05:00:00.000Z 2022-01-01T06:00:00.000Z 2022-01-01T07:00:00.000Z 2022-01-01T08:00:00.000Z 2022-01-01T09:00:00.000Z 2022-01-01T10:00:00.000Z 2022-01-01T11:00:00.000Z 2022-01-01T12:00:00.000Z 2022-01-01T13:00:00.000Z 2022-01-01T14:00:00.000Z 2022-01-01T15:00:00.000Z 2022-01-01T16:00:00.000Z 2022-01-01T17:00:00.000Z 2022-01-01T18:00:00.000Z 2022-01-01T19:00:00.000Z 2022-01-01T20:00:00.000Z 2022-01-01T21:00:00.000Z 2022-01-01T22:00:00.000Z 2022-01-01T23:00:00.000Z 2022-01-02T00:00:00.000Z <p>... and add some random generated data</p> <pre><code>SELECT random() as rand_figures, *\nFROM generate_series('2022-01-01','2022-01-02', INTERVAL '1 hour');\n</code></pre> rand_figures generate_series 0.203633273951709 2022-01-01T00:00:00.000Z 0.571097886189818 2022-01-01T01:00:00.000Z 0.629665858577937 2022-01-01T02:00:00.000Z 0.0612306422553957 2022-01-01T03:00:00.000Z 0.431237444281578 2022-01-01T04:00:00.000Z 0.229508123826236 2022-01-01T05:00:00.000Z 0.867487183306366 2022-01-01T06:00:00.000Z 0.758365222252905 2022-01-01T07:00:00.000Z 0.155569355469197 2022-01-01T08:00:00.000Z 0.786357307806611 2022-01-01T09:00:00.000Z 0.284404154401273 2022-01-01T10:00:00.000Z 0.367461221758276 2022-01-01T11:00:00.000Z 0.754724379163235 2022-01-01T12:00:00.000Z 0.0396546637639403 2022-01-01T13:00:00.000Z 0.276610609609634 2022-01-01T14:00:00.000Z 0.96564608765766 2022-01-01T15:00:00.000Z 0.127415937371552 2022-01-01T16:00:00.000Z 0.110610570758581 2022-01-01T17:00:00.000Z 0.764237959869206 2022-01-01T18:00:00.000Z 0.24844411527738 2022-01-01T19:00:00.000Z 0.0547867906279862 2022-01-01T20:00:00.000Z 0.977096977643669 2022-01-01T21:00:00.000Z 0.677903080359101 2022-01-01T22:00:00.000Z 0.173856796696782 2022-01-01T23:00:00.000Z 0.896873883903027 2022-01-02T00:00:00.000Z"},{"location":"SQL/#covert-datetimedatetimestamp-to-string","title":"Covert datetime/date/timestamp to string","text":"<pre><code>SELECT TO_CHAR(TIMESTAMP '2023-01-01 05:00:00', 'YYYY-MM-DD');\n</code></pre>"},{"location":"SQL/#postgres-upsert-insert-intoon-conflict-do-nothingupdate-set","title":"Postgres upsert (INSERT INTO...ON CONFLICT... DO NOTHING/UPDATE SET...)","text":"<p>DO NOTHING (<code>email</code> is unique)</p> <pre><code>INSERT INTO users (email, username, tel)\nVALUES\n    ('user_one@email.com', 'i_am_user_one', '0123456789'),\n    ('user_two@email.com', 'i_am_user_two', '9876543210')\nON CONFLICT (email)\nDO NOTHING;\n</code></pre> <p>DO UPDATE SET (<code>email</code> is unique)</p> <pre><code>INSERT INTO users (email, username, tel)\nVALUES\n    ('user_one@email.com', 'i_am_user_one', '0123456789'),\n    ('user_two@email.com', 'i_am_user_two', '9876543210')\nON CONFLICT (name)\nDO UPDATE SET username = EXCLUDED.username, tel = EXCLUDED.tel;\n</code></pre>"},{"location":"airflow/","title":"Airflow","text":""},{"location":"airflow/#run-with-docker","title":"Run with Docker","text":"<p>Airflow Installlation with docker images</p>"},{"location":"airflow/#docker-compose-file-w-postgres-mongodb","title":"docker compose file (w/ postgres + mongodb)","text":"<pre><code># Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\n#\n# WARNING: This configuration is for local development. Do not use it in a production deployment.\n#\n# This configuration supports basic configuration using environment variables or an .env file\n# The following variables are supported:\n#\n# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\n#                                Default: apache/airflow:2.5.3\n# AIRFLOW_UID                  - User ID in Airflow containers\n#                                Default: 50000\n# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.\n#                                Default: .\n# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\n#\n# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\n#                                Default: airflow\n# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\n#                                Default: airflow\n# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\n#                                Use this option ONLY for quick checks. Installing requirements at container\n#                                startup is done EVERY TIME the service is started.\n#                                A better way is to build a custom image or extend the official image\n#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.\n#                                Default: ''\n#\n# Feel free to modify this file to suit your needs.\n---\nversion: \"3.8\"\nx-airflow-common: &amp;airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.3}\n  build: .\n  environment: &amp;airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    # For backward compatibility, with Airflow &lt;2.3\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: \"\"\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \"true\"\n    AIRFLOW__CORE__LOAD_EXAMPLES: \"true\"\n    AIRFLOW__API__AUTH_BACKENDS: \"airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session\"\n    # yamllint disable rule:line-length\n    # Use simple http server on scheduler for health checks\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n    # yamllint enable rule:line-length\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: \"true\"\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on: &amp;airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n    mongo:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n      TEST_APP_USER: test_app_user\n      TEST_APP_PASSWORD: test_app\n      TEST_APP_DB: test_app\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n      - ./init_scripts/sql/init_db.sh:/docker-entrypoint-initdb.d/init_db.sh\n      - ./init_scripts/sql:/sql\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 10s\n      retries: 5\n      start_period: 5s\n    restart: always\n\n  mongo:\n    image: mongo:7.0.0\n    restart: always\n    healthcheck:\n      test: [\"CMD\", \"mongosh\", \"--eval\", \"db.adminCommand('ping')\"]\n      interval: 5s\n      timeout: 5s\n      retries: 3\n      start_period: 5s\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: mongo_test_db\n      MONGO_INITDB_ROOT_PASSWORD: mongo_test_db\n    ports:\n      - \"27017:27017\"\n\n  redis:\n    image: redis:latest\n    expose:\n      - 6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n    restart: always\n\n  airflow-webserver:\n    &lt;&lt;: *airflow-common\n    command: webserver\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-scheduler:\n    &lt;&lt;: *airflow-common\n    command: scheduler\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-worker:\n    &lt;&lt;: *airflow-common\n    command: celery worker\n    healthcheck:\n      test:\n        - \"CMD-SHELL\"\n        - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    environment:\n      &lt;&lt;: *airflow-common-env\n      # Required to handle warm shutdown of the celery workers properly\n      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n      DUMB_INIT_SETSID: \"0\"\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-triggerer:\n    &lt;&lt;: *airflow-common\n    command: triggerer\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"',\n        ]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-init:\n    &lt;&lt;: *airflow-common\n    entrypoint: /bin/bash\n    # yamllint disable rule:line-length\n    command:\n      - -c\n      - |\n        function ver() {\n          printf \"%04d%04d%04d%04d\" $${1//./ }\n        }\n        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO &amp;&amp; gosu airflow airflow version)\n        airflow_version_comparable=$$(ver $${airflow_version})\n        min_airflow_version=2.2.0\n        min_airflow_version_comparable=$$(ver $${min_airflow_version})\n        if (( airflow_version_comparable &lt; min_airflow_version_comparable )); then\n          echo\n          echo -e \"\\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\\e[0m\"\n          echo \"The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!\"\n          echo\n          exit 1\n        fi\n        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n          echo\n        fi\n        one_meg=1048576\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n        warning_resources=\"false\"\n        if (( mem_available &lt; 4000 )) ; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( cpus_available &lt; 2 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( disk_available &lt; one_meg * 10 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if [[ $${warning_resources} == \"true\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n          echo \"Please follow the instructions to increase amount of resources available:\"\n          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n          echo\n        fi\n        mkdir -p /sources/logs /sources/dags /sources/plugins\n        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}\n        exec /entrypoint airflow version\n    # yamllint enable rule:line-length\n    environment:\n      &lt;&lt;: *airflow-common-env\n      _AIRFLOW_DB_UPGRADE: \"true\"\n      _AIRFLOW_WWW_USER_CREATE: \"true\"\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      _PIP_ADDITIONAL_REQUIREMENTS: \"\"\n    user: \"0:0\"\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}:/sources\n\n  airflow-cli:\n    &lt;&lt;: *airflow-common\n    profiles:\n      - debug\n    environment:\n      &lt;&lt;: *airflow-common-env\n      CONNECTION_CHECK_MAX_COUNT: \"0\"\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n    command:\n      - bash\n      - -c\n      - airflow\n\n  # You can enable flower by adding \"--profile flower\" option e.g. docker-compose --profile flower up\n  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n  # See: https://docs.docker.com/compose/profiles/\n  flower:\n    &lt;&lt;: *airflow-common\n    command: celery flower\n    profiles:\n      - flower\n    ports:\n      - \"5555:5555\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\nvolumes:\n  postgres-db-volume:\n</code></pre>"},{"location":"airflow/#dockerfile-running-airflow-with-additional-modulesrequirementstxt","title":"Dockerfile (Running airflow with additional modules/requirements.txt)","text":"<pre><code>FROM apache/airflow:2.7.0-python3.11\n\nUSER root\nRUN apt-get update \\\n    &amp;&amp; apt-get install -y --no-install-recommends \\\n    vim \\\n    &amp;&amp; apt-get autoremove -yqq --purge \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\nUSER airflow\n\nCOPY requirements.txt /\nRUN pip install --no-cache-dir \"apache-airflow==${AIRFLOW_VERSION}\" -r /requirements.txt\n</code></pre>"},{"location":"airflow/#populatesql-migration-on-postgres-docker-start","title":"Populate/SQL migration on Postgres docker start","text":"<pre><code>      - ./init_scripts/sql/init_db.sh:/docker-entrypoint-initdb.d/init_db.sh\n      - ./init_scripts/sql:/sql\n</code></pre> <p>Add SQL scripts to the <code>/init_scripts/sql</code> directory e.g.:</p> <pre><code>-- ./init_scripts/sql/001-init_tables.sql\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\nCREATE TABLE IF NOT EXISTS departments (\n    id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\n    name VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\n\nCREATE TABLE IF NOT EXISTS employees (\n    id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\n    email VARCHAR(255) UNIQUE NOT NULL,\n    first_name VARCHAR(255) NOT NULL,\n    last_name VARCHAR(255) NOT NULL,\n    department_id uuid NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL,\n    FOREIGN KEY (department_id) REFERENCES departments(id) ON DELETE CASCADE\n);\n\nINSERT INTO\n    departments(name)\nVALUES\n    ('HR'), ('Finance');\n\nINSERT INTO\n    employees(email, first_name, last_name, department_id)\nVALUES\n    ('mary@test_co.com', 'Mary', 'Smith', (SELECT id from departments WHERE name='Finance')),\n    ('dave@test_co.com', 'Dave', 'Cole', (SELECT id from departments WHERE name='Finance')),\n    ('jane@test_co.com', 'Jane', 'Hills', (SELECT id from departments WHERE name='Finance')),\n    ('john@test_co.com', 'John', 'Doe', (SELECT id from departments WHERE name='HR'));\n</code></pre> <p>Add DB migrations files or SQL commands to <code>init_db.sh</code></p> <pre><code>#!/bin/bash\n\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" -d \"$POSTGRES_DB\"  &lt;&lt;-EOSQL\n  CREATE USER $TEST_APP_USER WITH PASSWORD '$TEST_APP_PASSWORD';\n    CREATE DATABASE $TEST_APP_DB;\n    GRANT ALL PRIVILEGES ON DATABASE $TEST_APP_DB TO $TEST_APP_USER;\nEOSQL\n\npsql -U \"$TEST_APP_USER\" -d \"$TEST_APP_DB\" -a -f /sql/001-init_tables.sql\n</code></pre>"},{"location":"docker/","title":"Docker","text":"<p>Docker cheat sheet</p>"},{"location":"docker/#create-and-start-containers","title":"Create and start containers","text":"<pre><code>docker-compose up\n</code></pre> <p>To run in background</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"docker/#stop-containers","title":"Stop containers","text":"<pre><code>docker-compose down\n</code></pre> <p>...and remove stopped containers</p> <pre><code>docker-compose down --remove-orphans\ndocker compose down --volumes --remove-orphans\n</code></pre>"},{"location":"docker/#docker-entrypointsh","title":"docker-entrypoint.sh","text":"<p>** For Linux ** If you want to access host from a docker container, you can find out more on how to do it here</p>"},{"location":"docker/#postgres","title":"Postgres","text":"<p>** <code>docker-compose.yml</code> example **</p> <pre><code>version: \"3.9\"\n\nservices:\n  postgres:\n    image: postgres:latest\n    restart: always\n    environment:\n      POSTGRES_USER: db_user\n      POSTGRES_PASSWORD: db_password\n      POSTGRES_DB: db_name\n    ports:\n      - \"5432:5432\"\n</code></pre> <p>Postgres in container to run with different port number:</p> <pre><code>ports:\n  - \"5434:5434\"\n\ncommand: -p 5434\n</code></pre> <p>Healthcheck on postgres container:</p> <pre><code>postgres:\n    ...\n  healthcheck:\n    test: [\"CMD-SHELL\", \"pg_isready -U db_user -d db_name -p 5432\"]\n    interval: 10s\n    timeout: 5s\n    retries: 5\n</code></pre>"},{"location":"docker/#flask","title":"Flask","text":"<p><code>Dockerfile</code></p> <pre><code>FROM python:3.13.1-slim\n\nRUN apt\u2212get \u2212y update\nRUN apt\u2212get install \u2212y pip3 build\u2212essential\n\nWORKDIR /app\nCOPY ./requirements.txt /app\nCOPY . ./app\nRUN pip install -r requirements.txt\n\nEXPOSE 5000\n# where the flask app initiate\nENV FLASK_APP=/app/my_app/app.py\n# or `production`, `uat` etc\nENV FLASK_ENV=development\nENV FLASK_DEBUG=1\nENV FLASK_RUN_PORT=5000\n\n# if not using `docker-entrypoint` mentioned earlier\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\n# or with gunicorn\n# CMD [\"gunicorn\", \"-b\", \":5000\", \"my_app.app:app\"]\n</code></pre> <p>add <code>python3 -m flask run</code> to the end of <code>docker-entrypoint.sh</code> if using <code>ENTRYPOINT</code></p> <p>Built the Docker image</p> <pre><code>docker build -t my-flask-app .\n</code></pre> <p>and run the container:</p> <pre><code>docker run -p 5000:5000 my-flask-app\n</code></pre>"},{"location":"docker/#with-docker-compose","title":"with docker compose","text":"<p>Flask + Postgres + Celery w/ Redis (Worker + Beat + Flower)</p> <pre><code>version: \"3.9\"\n\nservices:\n  flask_app:\n    build:\n      context: .\n    environment:\n      FLASK_ENV: development\n      FLASK_APP: /app/my_app/app.py\n      FLASK_DEBUG: 1\n      FLASK_RUN_PORT: 5000\n    entrypoint: ./dev-entrypoint.sh\n    volumes:\n      - .:/app\n    ports:\n      - 5000:5000\n    links:\n      - postgres\n      - redis\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n\n  postgres:\n    image: postgres:latest\n    environment:\n      POSTGRES_USER: db_user\n      POSTGRES_PASSWORD: db_password\n      POSTGRES_DB: db_name\n    ports:\n      - \"5432:5432\"\n    healthcheck:\n      test:[\"CMD-SHELL\", \"pg_isready -U db_user -d db_name\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  redis:\n    image: redis:latest\n    ports:\n      - \"6379:6379\"\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\"]\n\n  celery-worker:\n    build:\n      context: .\n    hostname: worker\n    entrypoint: celery\n    command: -A my_app.celery worker --loglevel=info  # change `my_app.celery` to where celery is init\n    volumes:\n      - .:/app\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n\n  celery-beat:\n    build:\n      context: .\n    hostname: beat\n    entrypoint: celery\n    command: -A my_app.celery beat --loglevel=info\n    volumes:\n      - .:/app\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n\n  celery-flower:\n    build:\n      context: .\n    hostname: flower\n    entrypoint: celery\n    command: -A my_app.celery flower --loglevel=info\n    volumes:\n      - .:/app\n        ports:\n      - 5555:5555\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n</code></pre>"},{"location":"git/","title":"Git","text":""},{"location":"git/#multi-git-accounts-access","title":"Multi git accounts access","text":"<p>If you work on different projects for different companies, or just want to separate company work from your own personal projects. You can use <code>includeIf</code> to tell git which identity/account to use depending on the directory path - for example, projects under <code>~/work/</code> folder are linked to the work account, <code>~/personal/</code> ones are linked to personal account</p> <p>First, in <code>~/.gitconfig</code> we do</p> <pre><code>[includeIf \"gitdir:~/work/\"]\n    path = .gitconfig-work\n[includeIf \"gitdir:~/personal/\"]\n    path = .gitconfig-personal\n</code></pre> <p>then edit/create (if haven't got one) <code>~/.gitconfig-personal</code> and add the git config of your personal account</p> <pre><code>[user]\nname = personalAccName\nemail = personal-acc@email-address-here.com\n</code></pre> <p>do the same for <code>~/.gitconfig-work</code> but with your work account info</p> <pre><code>[user]\nname = workAccName\nemail = work-acc@email-address-here.com\n</code></pre>"},{"location":"git/#add-remote-repo","title":"Add remote repo","text":"<pre><code>git remote add origin https://github.com/OWNER/REPOSITORY.git\n</code></pre>"},{"location":"git/#list-existing-remotes","title":"List existing remotes","text":"<pre><code>git remote -v\n</code></pre>"},{"location":"git/#change-remote-repos-url","title":"Change remote repo's URL","text":"<pre><code>git remote set-url origin https://github.com/OWNER/REPOSITORY.git\n</code></pre>"},{"location":"git/#rebse-branch-to-another","title":"Rebse branch to another","text":"<pre><code>git rebase origin/branch_name\n</code></pre>"},{"location":"git/#squash-amend-reword-commits-with-interactive-rebase-git-rebase-i","title":"Squash, amend, reword commits with interactive rebase (<code>git rebase -i</code>)","text":""},{"location":"git/#reword-commit-message","title":"Reword commit message","text":"<p>For example, I want to reword one of the last 3 commits</p> <pre><code>git rebase -i HEAD~3\n</code></pre> <p>will output the following</p> <pre><code>pick c2a4d95 reword me please\npick 2a4e409 update SQL query to insert new employee records\npick 08c8836 Refactor and clean up employee class\n\n# Rebase 8d8a4b3..08c8836 onto 8d8a4b3 (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n</code></pre> <p>To reword commit message <code>c2a4d95</code>, we replace <code>pick</code> with <code>reword</code>/<code>r</code> for that commit, then save and exit</p> <pre><code>reword c2a4d95 reword me please\npick 2a4e409 update SQL query to insert new employee records\npick 08c8836 Refactor and clean up employee class\n</code></pre> <p>we can now amend the commit message on the next \"screen\" - update the message then save and exit again:</p> <pre><code>Update documentation\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n...\n</code></pre> <p>It should output a message similar to the one below if rebased successfully:</p> <pre><code>[detached HEAD 7a8daf8] Update documentation\n Date: Sat Apr 22 10:01:48 2023 +0100\n 1 file changed, 71 insertions(+)\n create mode 100644 docs/docker.md\nSuccessfully rebased and updated refs/heads/rebase-test.\n</code></pre>"},{"location":"git/#squash-commits","title":"Squash commits","text":"<p>Using the same example from <code>reword</code> above, we now want to squash the three commits into one and amend the first commit message again:</p> <pre><code>reword 7a8daf8 Update documentation\nfixup 96dce5b update SQL query to insert new employee records\nfixup 467b4cc Refactor and clean up employee class\n</code></pre> <p>Here we use <code>fixup</code>/<code>f</code> instead of <code>squash</code>/<code>s</code> as we are discarding all the other commit messages except the top one.</p> <p>Again we should get <code>Successfully rebased and updated ...</code> if rebased successfully.</p>"},{"location":"localstack/","title":"Localstack","text":"<p>LocalStack provides a fully functional AWS cloud stack that can be run locally for development and testing purposes without using the actual AWS cloud services.</p>"},{"location":"localstack/#install-localstack-and-run","title":"Install Localstack and run","text":"<pre><code>pip install localstack\nlocalstack start -d\n</code></pre> <p>To get the status of each service</p> <pre><code>localstack status services\n</code></pre>"},{"location":"localstack/#run-via-docker-compose","title":"Run via Docker compose","text":"<p>** <code>docker-compose.yml</code> for Localstack S3, SQS and DynamoDB **</p> <pre><code>version: \"3.9\"\n\nservices:\n  localstack:\n    container_name: \"${LOCALSTACK_DOCKER_NAME-localstack_main}\"\n    image: localstack/localstack:latest\n    ports:\n      - \"4566:4566\"\n      - \"4510-4559:4510-4559\"\n    environment:\n      - DEBUG=1\n      - DOCKER_HOST=unix:///var/run/docker.sock\n            - SERVICES=s3,sqs,dynamodb\n    volumes:\n      - \"${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack\"\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n            - \"./data:/tmp/localstack\"\n        healthcheck:\n            test:\n        - CMD\n        - bash\n        - -c\n        - awslocal dynamodb list-tables\n          &amp;&amp; awslocal s3 ls\n          &amp;&amp; awslocal sqs list-queues\n            interval: 10s\n            timeout: 5s\n            retries: 5\n</code></pre>"},{"location":"localstack/#populating-data-into-localstack-s3-on-docker-compose-up","title":"Populating data into localstack S3 on <code>docker compose up</code>","text":"<p>Add these two lines to the <code>volumes</code> in the <code>docker-compose.yml</code> (assuming the data/files we want to upload to loaclstack S3 are in <code>s3_data</code> folder)</p> <pre><code>      - \"./.localstack:/etc/localstack/init/ready.d\"\n      - \"./s3_data:/s3_data\" # location of files used to pre-populate the Localstack S3 bucket\n</code></pre> <p>Then create <code>.localstack</code> directory, and inside this folder create a shell script (e.g. <code>create_and_populate_bucket.sh</code>) with commands to create and upload/synchronise the localstack S3 bucket with local data:</p> <pre><code>awslocal s3api create-bucket --bucket mock-bucket\nawslocal s3api put-bucket-policy --bucket mock-bucket --policy \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Sid\\\":\\\"PublicReadGetObject\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":\\\"*\\\",\\\"Action\\\":\\\"s3:GetObject\\\",\\\"Resource\\\":\\\"arn:aws:s3:::mock-bucket/*\\\"}]}\"\nawslocal s3 sync /s3_data s3://mock-bucket\n</code></pre> <p>The bucket policy is basically the inline version of this:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::mock-bucket/*\"\n        },\n    ]\n}\n</code></pre>"},{"location":"Javascript/datatable/","title":"Datatable","text":""},{"location":"Javascript/datatable/#reload-tables-with-new-data","title":"Reload table(s) with new data","text":"<pre><code>const reloadDatableData = () =&gt; {\n  const dataUrl = `/data-api-endpoint`;\n\n  fetch(dataUrl)\n    .then((response) =&gt; response.json())\n    .then((data) =&gt; {\n      const datatable = $(\"#datatable-id\").DataTable();\n      datatable.clear().rows.add(data).draw();\n      // in case of table width not working properly (responsive)\n      setTimeout(function () {\n        $(\"#datatable-id\").DataTable().columns.adjust().draw();\n      }, 100);\n    });\n};\n</code></pre>"},{"location":"Javascript/datatable/#rezise-tables-to-fit-container-in-bootstrap-v5-tabs","title":"Rezise table(s) to fit container in bootstrap (v5) tabs","text":"<pre><code>const initTabsEvts = () =&gt; {\n  const tabEl = document.querySelectorAll('a[data-bs-toggle=\"tab\"]');\n  tabEl.forEach((ele) =&gt; {\n    ele.addEventListener(\"shown.bs.tab\", function (event) {\n      const datatables = document.querySelector(\".tab-pane.active .datatable\");\n      datatables.forEach((datatable) =&gt; {\n        setTimeout(function () {\n          $(datatable).DataTable().columns.adjust().draw();\n        }, 50);\n      });\n    });\n  });\n};\n</code></pre>"},{"location":"Javascript/html-to-image/","title":"Html to image","text":""},{"location":"Javascript/html-to-image/#generate-an-image-from-a-dom-node-and-exportdownload","title":"Generate an image from a DOM node (and export/download)","text":"<p>Follow html-to-image GitHub repo for install instruction or get min.js from cdnjs</p> <p>E.g. trying to generate and download an image of <code>div#screenshot-me</code>:</p> <pre><code>&lt;div id=\"screenshot-me\"&gt;\n  &lt;div&gt;\n    &lt;img src=\"https://picsum.photos/id/29/536/354\" /&gt;\n  &lt;/div&gt;\n  &lt;p&gt;\n    Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo\n    ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis\n    parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec,\n    pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec\n    pede justo, fringilla vel, aliquet nec, vulputate eget, arcu.\n  &lt;/p&gt;\n  &lt;p&gt;\n    In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam\n    dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus.\n    Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo\n    ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem\n    ante, dapibus in, viverra quis, feugiat a, tellus.\n  &lt;/p&gt;\n  &lt;p&gt;\n    Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean\n    imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies\n    nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum\n    rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum.\n    Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem.\n  &lt;/p&gt;\n&lt;/div&gt;\n\n&lt;button\n  id=\"screenshot-btn\"\n  onclick=\"takeCardScreenShot('screenshot-me', 'screenshot.png')\"\n&gt;\n  Take a screenshot!\n&lt;/button&gt;\n</code></pre> <p>JavaSrcipt to generate the image and trigger download (html-to-image v 1.11.11)</p> <pre><code>const takeScreenShot = (eleId, filename) =&gt; {\n  htmlToImage\n    .toCanvas(document.getElementById(eleId), {\n      quality: 1,\n      backgroundColor: \"#FFFFFF\",\n    })\n    .then((canvas) =&gt; {\n      imgSaveAs(canvas.toDataURL(\"image/png\"), `${filename}.png`);\n    })\n    .catch((error) =&gt; {\n      console.log(error);\n    });\n};\n\nconst imgSaveAs = (uri, filename) =&gt; {\n  const link = document.createElement(\"a\");\n  if (typeof link.download === \"string\") {\n    link.href = uri;\n    link.download = filename;\n    document.body.appendChild(link);\n    link.click();\n    document.body.removeChild(link);\n  } else {\n    window.open(uri);\n  }\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/","title":"Javascript notes","text":""},{"location":"Javascript/javascript%20notes/#copy-to-clipboard","title":"Copy to clipboard","text":"<pre><code>const copyToClipboard = (txt) =&gt; {\n  try {\n    navigator.clipboard.writeText(url);\n  } catch (e) {\n    unsecuredCopyToClipboard(txt);\n  }\n  showToastNotification(\"Copied to clipboard\", \"success\");\n};\n\nconst unsecuredCopyToClipboard = (text) =&gt; {\n  const ta = document.createElement(\"textarea\");\n  ta.value = text;\n  document.body.appendChild(ta);\n  ta.focus();\n  ta.select();\n  try {\n    document.execCommand(\"copy\");\n  } catch (err) {\n    console.error(\"Unable to copy to clipboard\", err);\n  }\n  document.body.removeChild(ta);\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#fetch","title":"fetch","text":"<pre><code>const fetchExample = (url) =&gt; {\n  fetch(url, {\n    method: \"POST\",\n    headers: {\n      Accept: \"application/json\",\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({ ... }),\n  })\n    .then((res) =&gt; {\n      if (res.ok) {\n        return res.json();\n      }\n      return Promise.reject(res);\n    })\n    .then((body) =&gt; {\n      ...\n    })\n    .catch((error) =&gt; {\n      ...\n    })\n    .finally(() =&gt; {\n     ...\n    });\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#download-blobfile-with-fetch","title":"Download blob/file with fetch","text":"<pre><code>const downloadFile = (fileUrl) =&gt; {\n  fetch(fileUrl, {\n    headers: {\n      Accept: \"application/json\",\n      \"Content-Type\": \"application/json\",\n    },\n  })\n    .then((response) =&gt; {\n      if (!response.ok) {\n        return response.json().then((body) =&gt; {\n          // error handling\n          if (body.status == \"error\") {\n            return downloadErrorHandler();\n          }\n          throw new Error(\"HTTP status \" + response.status);\n        });\n      }\n      return response.blob().then((blob) =&gt; {\n        filename = extractFilenameFromHeaders(response.headers);\n        return downloadBlob(blob, filename);\n      });\n    })\n    .catch((error) =&gt; {\n      console.log(error);\n    });\n};\n\nconst downloadBlob = (blob, filename) =&gt; {\n  const link = document.createElement(\"a\");\n  link.href = window.URL.createObjectURL(blob);\n  link.download = filename;\n  link.click();\n  link.remove();\n};\n\n// Extract filename from headers\nconst extractFilenameFromHeaders = (headers) =&gt; {\n  const header = headers.get(\"Content-Disposition\");\n  const parts = header.split(\";\");\n  return parts[1].split(\"=\")[1];\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#open-url-on-same-tab","title":"Open url on same tab","text":"<pre><code>window.location.href = url;\n</code></pre>"},{"location":"Javascript/javascript%20notes/#open-url-on-new-tab","title":"Open url on new tab","text":"<pre><code>window.open(url, \"_blank\");\n</code></pre>"},{"location":"Javascript/javascript%20notes/#axios","title":"Axios","text":"<p>Axios</p>"},{"location":"Javascript/plotly/","title":"Plotly","text":""},{"location":"Javascript/plotly/#plotly-js","title":"Plotly JS","text":"<p>Placeholder for chart in HTML:</p> <pre><code>&lt;div id=\"chart-id-here\" class=\"plotly-charts\"&gt;&lt;/div&gt;\n\ninitChart(\"chart-id-here\");\n</code></pre> <p>Javascript to fetch chart data from API endpoint</p> <pre><code>const initChart = (eleId) =&gt; {\n  fetch(\"/api-endpoint-for-chart-data\")\n    .then((response) =&gt; response.json())\n    .then((data) =&gt; {\n      const chartData = data.chart_data;\n      const plotConfigData = data.config_data;\n      plotlyChart(eleId, chartData, null, plotConfigData);\n    });\n};\n</code></pre> <pre><code>const plotlyChartLayout = (title, opts) =&gt; {\n  let layout = $.extend(\n    {\n      title: {\n        text: title,\n        font: {\n          size: 12,\n        },\n      },\n      xaxis: {\n        tickfont: {\n          size: 11,\n        },\n      },\n      yaxis: {\n        automargin: true,\n        tickfont: {\n          size: 11,\n        },\n      },\n      showlegend: true,\n      legend: {\n        font: {\n          size: 10,\n        },\n      },\n      bargap: 0.2,\n      font: { color: \"#4e4e4e\" },\n    },\n    opts\n  );\n\n  return layout;\n};\n\nconst plotlyChart = (\n  targetEle,\n  data,\n  title,\n  opts = null,\n  configOpts = null,\n  refreshData = false\n) =&gt; {\n  const layout = plotlyChartLayout(title, opts);\n  const imgFilename = opts.filename ? opts.filename : \"plot-image\";\n  const config = $.extend(\n    {\n      responsive: true,\n      displayModeBar: true,\n      // to hide the buttons/chart functions we don't want\n      modeBarButtonsToRemove: [\n        \"zoom2d\",\n        \"pan2d\",\n        \"select2d\",\n        \"lasso2d\",\n        \"zoomIn2d\",\n        \"zoomOut2d\",\n        \"autoScale2d\",\n        \"resetScale2d\",\n      ],\n      toImageButtonOptions: {\n        format: \"png\",\n        filename: imgFilename,\n        scale: 2,\n      },\n      displaylogo: false,\n    },\n    configOpts\n  );\n  if (refreshData) {\n    Plotly.react(targetEle, data, layout, config);\n  } else {\n    Plotly.newPlot(targetEle, data, layout, config);\n  }\n};\n</code></pre>"},{"location":"Javascript/plotly/#rezise-charts-to-fit-container-in-bootstrap-v5-tabs","title":"Rezise chart(s) to fit container in bootstrap (v5) tabs","text":"<pre><code>const initTabsEvts = () =&gt; {\n  const tabEl = document.querySelectorAll('a[data-bs-toggle=\"tab\"]');\n  tabEl.forEach((ele) =&gt; {\n    ele.addEventListener(\"shown.bs.tab\", function (event) {\n      const charts = document.querySelectorAll(\n        \".tab-pane.active .plotly-charts\"\n      );\n      charts.forEach((chart) =&gt; {\n        Plotly.relayout(chart, { autosize: true });\n      });\n    });\n  });\n};\n</code></pre>"},{"location":"Javascript/tabulator/","title":"Tabulator","text":"<p>Tabulator</p> <pre><code>const initTable = (dataUrl) =&gt; {\n  const table = new Tabulator(\"#table\", {\n    ajaxURL: dataUrl,\n    layout: \"fitColumns\",\n    index: \"id\",\n    maxHeight: \"50vh\",\n    resizableColumns: false,\n    placeholder: \"No Data Available\",\n    initialSort: [{ column: \"timestamp\", dir: \"desc\" }],\n    columns: [\n      {\n        field: \"id\",\n        title: \"ID\",\n        sorter: \"string\",\n        visible: false,\n      },\n      {\n        field: \"timestamp\",\n        title: \"Timestamp\",\n        sorter: \"datetime\",\n        formatter: \"datetime\",\n        formatterParams: {\n          inputFormat: \"yyyy-MM-dd HH:mm:ss\",\n          outputFormat: \"yyyy-MM-dd HH:mm:ss\",\n        },\n        headerFilter: false,\n      },\n\n      {\n        field: \"date_val\",\n        title: \"Date value\",\n        sorter: \"date\",\n        sorterParams: {\n          alignEmptyValues: \"bottom\",\n          format: \"yyyy-MM-dd\",\n        },\n        formatter: \"date\",\n        formatterParams: {\n          inputFormat: \"yyyy-MM-dd\",\n          outputFormat: \"yyyy-MM-dd\",\n        },\n        headerFilter: true,\n      },\n      {\n        field: \"boolean_val\",\n        title: \"Boolean value\",\n        sorter: \"boolean\",\n        formatter: \"tickCross\",\n        headerFilter: \"tickCross\",\n        headerFilterParams: { tristate: true },\n        hozAlign: \"center\",\n        formatterParams: {\n          allowEmpty: true,\n          allowTruthy: true,\n          tickElement: \"Yes\",\n          crossElement: false,\n        },\n      },\n      {\n        field: \"icon\",\n        title: \"Icon\",\n        headerFilter: false,\n        hozAlign: \"center\",\n        formatter: iconLink,\n      },\n    ],\n  });\n};\n\nconst iconLink = (cell, formatterParams) =&gt; {\n  const val = cell.getValue();\n  if (!!val) {\n    return `&lt;i class=\"fe fe-link pointer\" data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" title=\"${val}\"&gt;&lt;/i&gt;`;\n  }\n  return \"\";\n};\n\nconst refreshTableDate = (dataUrl) =&gt; {\n  table.replaceData(dataUrl);\n};\n</code></pre>"},{"location":"Python/pandas/","title":"Pandas","text":""},{"location":"Python/pandas/#flatten-entityattributevalue-modelstables-using-pandas-dataframe","title":"Flatten entity\u2013attribute\u2013value models/tables using Pandas dataframe","text":"<p>users (entity/object table)</p> id name email 1 John Smith john.smith@example.com 2 Jane Smith jane.smith@example.com <p>page_config (attribute table)</p> id name 1 font-size 2 color 3 background-color <p>users_page_config (EAV table)</p> id user_id page_config_id value 1 1 1 12px 2 1 2 #3e3e3e 3 1 3 blue 4 2 1 16px 5 2 2 #000000 6 2 3 green <pre><code>import numpy as np\nimport pandas as pd\n\n\nclass EavFlattener:\n    def flatten_eav_table(self, data):\n        result_df = pd.DataFrame(data)\n        result_df = result_df.pivot(index=\"user_id\", columns='attribute')['attribute_value']\n        result_df = result_df.reset_index()\n        result_df = result_df.drop(np.nan, axis=1)\n        return result_df\n\n\ndata = [\n    {'user_id': 1, 'attribute': 'font-size', 'attribute_value': '12px'},\n    {'user_id': 1, 'attribute': 'color', 'attribute_value': '#3e3e3e'},\n    {'user_id': 1, 'attribute': 'background-color', 'attribute_value': 'blue'},\n    {'user_id': 2, 'attribute': 'font-size', 'attribute_value': '16px'},\n    {'user_id': 2, 'attribute': 'color', 'attribute_value': '#000000'},\n    {'user_id': 2, 'attribute': 'background-color', 'attribute_value': 'green'}\n]\n\nEavFlattener().flatten_eav_table(data)\n\n# Flattened EAV data\n# [{'user_id': 1, 'background-color': 'blue', 'color': '#3e3e3e', 'font-size': '12px'},\n#  {'user_id': 2, 'background-color': 'green', 'color': '#000000', 'font-size': '16px'}]\n</code></pre>"},{"location":"Python/pyJWT/","title":"pyJWT","text":"<p>PyJWT</p>"},{"location":"Python/pyJWT/#validate-and-return-user-with-aws-cognito-access-token","title":"Validate and return user with AWS cognito access token","text":"<pre><code>import json\n\nfrom typing import Optional\n\nimport jwt\nimport requests\n\nfrom jwt.algorithms import RSAAlgorithm\n\nfrom path.to.user_model import User\nfrom path.to.config import config\n\n\nclass UserAuthService:\n    def get_current_user(self, access_token: str = None) -&gt; Optional[User]:\n        valid_token, current_user = self.validate_and_return_credentials(access_token)\n        return current_user if (valid_token and current_user) else None\n\n    def get_public_access_keys(self) -&gt; str:\n        auth = config.auth\n        url = f\"https://cognito-idp.{auth.region}.amazonaws.com/{auth.identity_pool_id}/.well-known/jwks.json\"\n        response = requests.get(url)\n        return response.text\n\n    def find_public_key(self, kid: str) -&gt; tuple[bool, Optional[dict]]:\n        public_keys = self.get_public_access_keys()\n        public_keys = json.loads(public_keys)\n        public_keys = {key[\"kid\"]: key for key in public_keys[\"keys\"]}\n        matched_key = public_keys.get(kid)\n        return bool(matched_key), matched_key\n\n    def decode_access_token(self, public: dict, access_token: str, alg: str) -&gt; tuple[bool, dict]:\n        try:\n            public_key = RSAAlgorithm.from_jwk(json.dumps(public))\n            payload = jwt.decode(\n                access_token,\n                public_key,\n                algorithms=[alg],\n                verify=True,\n                options={\n                    \"verify_exp\": True,\n                    \"verify_nbf\": True,\n                    \"verify_iat\": False, # True\n                    \"verify_aud\": False, # True\n                    \"verify_iss\": True,\n                },\n                audience=config.auth.identity_pool_web_client,\n                issuer=f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\",\n            )\n        except jwt.exceptions.DecodeError as exc:\n            return False, {}\n\n        # This should be check if token_use is ID token and \"verify_aud\" in jwt decode\n        if payload[\"token_use\"] != \"access\":\n            return False, {}\n\n        if payload[\"client_id\"] != config.auth.identity_pool_web_client:\n            return False, {}\n\n        return True, payload\n\n    def validate_and_return_credentials(self, access_token: str) -&gt; tuple[bool, Optional[User]]:\n        valid_token, current_user = False, None\n\n        try:\n            headers = jwt.get_unverified_header(access_token)\n            valid_pub_key, user_public_key = self.find_public_key(headers.get(\"kid\"))\n            if not user_public_key:\n                return False, None\n\n            valid_signature, payload = self.decode_access_token(user_public_key, access_token, headers.get(\"alg\"))\n            if valid_pub_key and valid_signature:\n                valid_token = True\n                user_sub_id = payload.get(\"sub\")\n                current_user = User.find_by(guid=user_sub_id) if user_sub_id else None\n        except Exception as exc:\n                        return False, None\n\n        return valid_token, current_user\n</code></pre>"},{"location":"Python/pyJWT/#with-alb-token-verification-alb-beast-vulnerability","title":"With ALB token verification (ALB Beast vulnerability)","text":"<pre><code>import json\n\nfrom typing import Optional, Tuple\n\nimport jwt\nimport requests\n\nfrom insig.logging.logger import Logger\nfrom jwt.algorithms import RSAAlgorithm\n\nfrom app.config import config\nfrom path.to.user_model import User\n\n\nclass UserAuthService:\n    def get_current_user(self, access_token: str = None, data_token: str = None):\n        is_valid_alb_token = self.verify_alb_token(data_token)\n        if not is_valid_alb_token:\n            return None\n\n        valid_token, current_user = self.validate_access_token_and_return_credentials(\n            access_token\n        )\n        return current_user if (valid_token and current_user) else None\n\n\n    def get_public_access_keys(self, url: str) -&gt; Optional[str]:\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            return response.text\n        except Exception as exc:\n            logger.warning(\"Auth Failed: Invalid public key %s\", str(exc))\n            return None\n\n    def find_public_key(self, kid: str) -&gt; Tuple[bool, Optional[dict]]:\n        auth = config.auth\n        url = f\"https://cognito-idp.{auth.region}.amazonaws.com/{auth.identity_pool_id}/.well-known/jwks.json\"\n        public_keys = self.get_public_access_keys(url)\n        if not public_keys:\n            logger.warning(\"Auth Failed: Invalid cognito public key\")\n            return False, {}\n\n        public_keys = json.loads(public_keys)\n        public_keys = {key[\"kid\"]: key for key in public_keys[\"keys\"]}\n        matched_key = public_keys.get(kid)\n        return bool(matched_key), matched_key\n\n    def decode_access_token(self, public: dict, access_token: str, alg: str) -&gt; tuple[bool, dict]:\n        try:\n            public_key = RSAAlgorithm.from_jwk(json.dumps(public))\n            payload = jwt.decode(\n                access_token,\n                public_key,\n                algorithms=[alg],\n                verify=True,\n                options={\n                    \"verify_exp\": True,\n                    \"verify_nbf\": True,\n                    \"verify_iat\": False,\n                    \"verify_aud\": False,\n                    \"verify_iss\": True,\n                },\n                audience=config.auth.identity_pool_web_client,\n                issuer=f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\",\n            )\n        except jwt.exceptions.DecodeError as exc:\n            logger.warning(\"Auth Failed - JWT DecodeError: %s\", str(exc))\n            return False, {}\n\n        # This should be check if token_use is ID token and \"verify_aud\" in jwt decode\n        if payload[\"token_use\"] != \"access\":\n            return False, {}\n\n        if payload[\"client_id\"] != config.auth.identity_pool_web_client:\n            return False, {}\n\n        return True, payload\n\n    def verify_alb_token(self, data_token: str) -&gt; bool:\n        if not data_token:\n            return False\n\n        headers = jwt.get_unverified_header(data_token)\n        auth = config.auth\n        expected_signer = (\n            f\"arn:aws:elasticloadbalancing:{auth.region}:{auth.alb_id}:\"\n            f\"loadbalancer/app/{auth.load_balancer_name}/{auth.load_balancer_id}\"\n        )\n        if not headers.get(\"signer\") == expected_signer:\n            logger.warning(\"Auth Failed: Invalid ALB signer\")\n            return False\n\n        url = f'https://public-keys.auth.elb.{auth.region}.amazonaws.com/{headers.get(\"kid\")}'\n        public_key = self.get_public_access_keys(url)\n        if not public_key:\n            logger.warning(\"Auth Failed: Invalid ALB public key\")\n            return False\n\n        expected_issuer = f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\"\n        payload = jwt.decode(\n            data_token,\n            public_key,\n            algorithms=[headers[\"alg\"]],\n            verify=True,\n            options={\n                \"verify_exp\": True,\n                \"verify_nbf\": True,\n                \"verify_iat\": False,\n                \"verify_aud\": False,\n                \"verify_iss\": True,\n            },\n            audience=config.auth.identity_pool_web_client,\n            issuer=expected_issuer,\n        )\n\n        return payload.get(\"iss\") == expected_issuer\n\n    def validate_access_token_and_return_credentials(self, access_token: str) -&gt; Tuple[bool, Optional[str], bool]:\n        valid_token, user_sub_id, current_user = False, None, None\n        try:\n            headers = jwt.get_unverified_header(access_token)\n            valid_pub_key, user_public_key = self.find_public_key(headers.get(\"kid\"))\n            if not user_public_key:\n                logger.warning(\"Auth Failed: Key not found\")\n                return False, None, None\n\n            valid_signature, payload = self.decode_access_token(user_public_key, access_token, headers.get(\"alg\"))\n            if valid_pub_key and valid_signature:\n                valid_token = True\n                user_sub_id = payload.get(\"sub\")\n                current_user = User.find_by(guid=user_sub_id) if user_sub_id else None\n        except Exception as exc:\n            logger.warning(\"Auth Failed: %s\", str(exc))\n\n        return valid_token, current_user\n</code></pre>"},{"location":"Python/python%20notes/","title":"Python notes","text":""},{"location":"Python/python%20notes/#logging-class","title":"Logging class","text":"<pre><code>import logging\n\n\nclass Logger:\n    def __init__(self):\n            self._logger = None\n\n    def setup_logger(self):\n            logger = logging.getLogger(\"root\")\n\n            if logger.handlers:\n                return logger\n\n            logger.setLevel(logging.INFO)\n            formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(module)s %(funcName)s - %(message)s\")\n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n\n            return logger\n\n    def get_logger(self):\n            if self._logger is None:\n                self._logger = self.setup_logger()\n\n            return self._logger\n</code></pre> <p>To use:</p> <pre><code>from path.to.logger import Logger\n\nlogger = Logger().get_logger()\n</code></pre>"},{"location":"Python/python%20notes/#creating-decorator","title":"Creating decorator","text":"<pre><code>def logged_in_user(function) -&gt; Any:\n    @wraps(function)\n    def get_logged_in_user(*args, **kwargs):\n        current_user = get_valid_current_user()\n        if not current_user:\n            return reject_access()\n\n        return function(*args, **kwargs, current_user=current_user)\n\n    return get_logged_in_user\n\n\n@app.post('/')\n@logged_in_user\ndef app_root_page(current_user):\n  ...\n</code></pre> <p>Decorator with params</p> <pre><code>def some_decorator(val) -&gt; Any:\n    def some_decorator_func(function) -&gt; Any:\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n                        some_bool = val &gt; 10\n                        # do other stuff\n            return function(*args, **kwargs, some_bool=some_bool)\n\n        return wrapper\n\n    return some_decorator_func\n</code></pre>"},{"location":"Python/python%20notes/#creating-batch-from-list","title":"creating batch from list","text":"<pre><code>def create_batch(self, obj_ls: list[\"ObjCls\"], batch_size: int):\n    for i in range(0, len(obj_ls), batch_size):\n        yield obj_ls[i : i + batch_size]\n</code></pre>"},{"location":"Python/python%20notes/#list-filtering","title":"List filtering","text":"<p>Supposing we need to filter out the results from the below list of dictionaries</p> <pre><code>exam_results = [\n    {\"name\": \"John\", \"score\": 70, \"subject\": \"German\"},\n    {\"name\": \"Jane\", \"score\": 80, \"subject\": \"Maths\"},\n    {\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"},\n    {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"},\n    {\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"},\n    {\"name\": \"Theo\", \"score\": 40, \"subject\": \"Chemistry\"},\n]\n</code></pre> <p>To retrieve exam results where <code>score</code> is higher than 65:</p> <pre><code>filtered_result = list(filter(lambda x: x[\"score\"] &gt; 65, exam_results))\nprint(filtered_result)\n# [{\"name\": \"John\", \"score\": 70, \"subject\": \"German\"}, {\"name\": \"Jane\", \"score\": 80, \"subject\": \"Maths\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre> <p>To retrieve exam results where <code>score</code> is equal or higher than 50 and subject is either <code>Chemistry</code>, <code>French</code>, <code>Graphics</code>:</p> <pre><code>filtered_subject_results = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] in ([\"Chemistry\", \"French\", \"Graphics\"]), exam_results))\n\nprint(filtered_subject_results)\n# [{\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}, {\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}]\n</code></pre> <p>...and with <code>subject</code> in specific order using for-loop</p> <pre><code>filtered_subject_results = []\nfor subject_name in [\"Chemistry\", \"French\", \"Graphics\"]:\n    result = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] == subject_name, exam_results))\n    filtered_subject_results = filtered_subject_results + result\n\nprint(filtered_subject_results)\n# [{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}, {\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre> <p>...or with <code>sorted</code></p> <pre><code>filtered_subject_results = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] in ([\"Chemistry\", \"French\", \"Graphics\"]), exam_results))\nfiltered_subject_results = sorted(filtered_subject_results, key=lambda x: (x[\"subject\"]))\n\nprint(filtered_subject_results)\n# [{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}, {\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre>"},{"location":"Python/python%20notes/#sort-groupby-itertools","title":"sort &amp; groupby (itertools)","text":"<p>Assuming we have a list of dictionaries of students needed to be grouped by year-class:</p> <pre><code>students_ls = [\n    {\"name\": \"John\", \"email\": \"john@random_school.edu.uk\", \"year\": 1, \"class\": \"A\"},\n    {\"name\": \"Jane\", \"email\": \"jane@random_school.edu.uk\", \"year\": 1, \"class\": \"B\"},\n    {\"name\": \"Mary\", \"email\": \"mary@random_school.edu.uk\", \"year\": 2, \"class\": \"C\"},\n    {\"name\": \"Alex\", \"email\": \"alex@random_school.edu.uk\", \"year\": 2, \"class\": \"A\"},\n    {\"name\": \"Sam\", \"email\": \"sam@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n    {\"name\": \"Hannah\", \"email\": \"hannah@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n    {\"name\": \"Kim\", \"email\": \"kim@random_school.edu.uk\", \"year\": 3, \"class\": \"B\"},\n    {\"name\": \"Ted\", \"email\": \"ted@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n]\n</code></pre> <p>First, we have to sorted the list by year and class as the documentation states that the list need to be sorted before applying <code>groupby</code></p> <pre><code>sorted_students_ls = sorted(students_ls, key=lambda x: (x[\"year\"], x[\"class\"]))\nprint(sorted_students_ls)\n# [\n#   {'class': 'A', 'email': 'john@random_school.edu.uk', 'name': 'John', 'year': 1},\n#   {'class': 'B', 'email': 'jane@random_school.edu.uk', 'name': 'Jane', 'year': 1},\n#   {'class': 'A', 'email': 'alex@random_school.edu.uk', 'name': 'Alex', 'year': 2},\n#   {'class': 'C', 'email': 'mary@random_school.edu.uk', 'name': 'Mary', 'year': 2},\n#   {'class': 'A', 'email': 'sam@random_school.edu.uk', 'name': 'Sam', 'year': 3},\n#   {'class': 'A', 'email': 'hannah@random_school.edu.uk', 'name': 'Hannah', 'year': 3},\n#   {'class': 'A', 'email': 'ted@random_school.edu.uk', 'name': 'Ted', 'year': 3},\n#   {'class': 'B', 'email': 'kim@random_school.edu.uk', 'name': 'Kim', 'year': 3}\n# ]\n</code></pre> <p>Then apply <code>groupby</code>:</p> <pre><code>grouped_data_single_line = {f\"{k[0]}-{k[1]}\": list(students) for k, students in itertools.groupby(sorted_students_ls, key=lambda x: (x[\"year\"], x[\"class\"]))}\nprint(grouped_data_single_line)\n# {\n#   \"1-A\": [{'class': 'A', 'email': 'john@random_school.edu.uk', 'name': 'John', 'year': 1}],\n#   \"1-B\": [{'class': 'B', 'email': 'jane@random_school.edu.uk', 'name': 'Jane', 'year': 1}],\n#   \"2-A\": [{'class': 'A', 'email': 'alex@random_school.edu.uk', 'name': 'Alex', 'year': 2}],\n#   \"2-C\": [{'class': 'C', 'email': 'mary@random_school.edu.uk', 'name': 'Mary', 'year': 2}],\n#   \"3-A\": [{'class': 'A', 'email': 'sam@random_school.edu.uk', 'name': 'Sam', 'year': 3},\n#           {'class': 'A', 'email': 'hannah@random_school.edu.uk', 'name': 'Hannah', 'year': 3},\n#           {'class': 'A', 'email': 'ted@random_school.edu.uk', 'name': 'Ted', 'year': 3}],\n#   \"3-B\": [{'class': 'B', 'email': 'kim@random_school.edu.uk', 'name': 'Kim', 'year': 3}]\n# }\n</code></pre> <p>or below if require data formatting</p> <pre><code>grouped_data = {}\nfor k, students in itertools.groupby(sorted_students_ls, key=lambda x: (x[\"year\"], x[\"class\"])):\n    data_key = f\"{k[0]}-{k[1]}\"\n    grouped_data[data_key] = [{\"name\": student[\"name\"], \"email\": student[\"email\"]} for student in list(students)]\n\nprint(grouped_data_single_line)\n# {\n#     '1-A': [{'email': 'john@random_school.edu.uk', 'name': 'John'}],\n#     '1-B': [{'email': 'jane@random_school.edu.uk', 'name': 'Jane'}],\n#     '2-A': [{'email': 'alex@random_school.edu.uk', 'name': 'Alex'}],\n#     '2-C': [{'email': 'mary@random_school.edu.uk', 'name': 'Mary'}],\n#     '3-A': [{'email': 'sam@random_school.edu.uk', 'name': 'Sam'},\n#             {'email': 'hannah@random_school.edu.uk', 'name': 'Hannah'},\n#             {'email': 'ted@random_school.edu.uk', 'name': 'Ted'}],\n#     '3-B': [{'email': 'kim@random_school.edu.uk', 'name': 'Kim'}]\n# }\n</code></pre>"},{"location":"Python/python%20notes/#dates-related","title":"Dates related","text":"<p>** Get last day of week from a given date **</p> <pre><code>from datetime import date, datetime, timedelta\n\ndef get_week_end(date_input: date):\n    # use 6 for week ending on Sunday\n    return date_input + timedelta(days=5 - date_input.weekday())\n</code></pre> <p>** Get last day of month from a given date **</p> <pre><code>import bisect\nimport calendar\n\nfrom datetime import date, datetime, timedelta\n\ndef get_month_end(date_input: date):\n    last_day_of_month = calendar.monthrange(date_input.year, date_input.month)[1]\n    return date(date_input.year, date_input.month, last_day_of_month)\n</code></pre> <p>** Get start &amp; end of quarter from a given date **</p> <pre><code>import bisect\nimport calendar\n\nfrom datetime import date, datetime, timedelta\n\ndef get_quarter_end(date_input: date):\n    quarter_ends = [date(date_input.year, month, 1) + timedelta(days=-1) for month in (4, 7, 10)]\n    quarter_ends.append(date(date_input.year + 1, 1, 1) + timedelta(days=-1))\n    idx = bisect.bisect_left(quarter_ends, date_input)\n    return quarter_ends[idx]\n\n\ndef get_quarter_start(date_input: date):\n    quarter_start = [date(date_input.year, month, 1) for month in (1, 4, 7, 10)]\n    idx = bisect.bisect(quarter_start, date_input)\n    return quarter_start[idx - 1]\n</code></pre>"},{"location":"Python/python%20notes/#user-authorisationpermission","title":"User authorisation/permission","text":"<p>An tiny user authorisation/permission python/flask class inspired by Ruby's authorization library Pundit</p> <pre><code>import inspect\nimport re\n\nfrom flask import g, request\nfrom path.to.user.model import User\n\n\n@staticmethod\ndef to_snake_case(obj_str):\n    reg_match = re.compile(\"((?&lt;=[a-z0-9])[A-Z]|(?!^)(?&lt;!_)[A-Z](?=[a-z]))\")\n    return reg_match.sub(r\"_\\1\", obj_str).lower()\n\n\nclass UserAuth:\n    def __init__(self):\n        self._current_user = None\n\n    @property\n    def current_user(self):\n        if self._current_user is None:\n            # can replace with g.current_user or method to get current user\n            self._current_user = User.get(g.user_id)\n\n        return self._current_user\n\n    @classmethod\n    def get_policy_cls(cls, model_cls):\n        model_cls_name = model_cls.__name__\n        model_name_snake_case = to_snake_case(model_cls_name)\n        policy_path = f\"path.to.policies.{model_name_snake_case}_policy\"\n        policy_module = __import__(policy_path, fromlist=[f\"{model_cls_name}Policy\"])\n        policy_cls = getattr(policy_module, f\"{model_cls_name}Policy\")\n\n        return policy_cls\n\n    @classmethod\n    def authorised_action(cls, model_obj, action=None, *args, **kwargs):\n        model_cls = cls.get_model_class(model_obj)\n        model_policy = cls.get_policy_cls(model_cls)\n        action = action or request.method.lower()\n        return getattr(model_policy(self.current_user, model_obj), action)(*args, **kwargs)\n\n    @classmethod\n    def authorised_scope(cls, model_obj, *args, **kwargs):\n        model_cls = cls.get_model_class(model_obj)\n        model_policy = cls.get_policy_cls(model_cls)\n        return getattr(model_policy(self.current_user, model_cls), \"scope\")(*args, **kwargs)\n\n    @classmethod\n    def get_model_class(cls, model_obj):\n        if inspect.isclass(model_obj):\n            return model_obj\n\n        return model_obj.__class__\n</code></pre>"},{"location":"Python/python%20notes/#how-it-works","title":"How it works","text":"<p>Assuming we have a <code>User</code> class similar to the example below (with SQLAlchemy):</p> <pre><code># Path to SQLalchemy/Database config\nfrom app.setting.database import db\n\nclass UserRole(enum.Enum):\n    REGISTER = 1\n    ADMIN = 2\n\n\nclass User(db.Model):\n    # for dataclass, can do this instead of `__init__`\n    # id: uuid.UUID\n    # email: str\n    # first_name: str\n    # role: UserRole = UserRole.REGISTER\n\n    def __init__(self, email: str, name: str, role: int = UserRole.REGISTER) -&gt; None:\n        self.email = email\n        self.name = name\n        self.role = role\n\n    @classmethod\n    def create(cls, email: str, name: str, role: int = UserRole.REGISTER) -&gt; \"User\":\n        user = cls(email=email, name=name, role=role)\n        db.session.add(user)\n        db.session.commit()\n        return user\n\n    @classmethod\n    def get(cls, id: str) -&gt; \"User\":\n        return cls.query.get(id)\n\n    @classmethod\n    def all(cls) -&gt; list[\"User\"]:\n        return cls.query.all()\n\n    def update(self, name: str, email:str) -&gt; None:\n        self.name = name\n        self.email = email\n        db.session.commit()\n\n    @property\n    def is_admin(self) -&gt; bool:\n        return self.role == UserRole.ADMIN\n</code></pre> <p>And <code>UserPolicy</code> - assuming only admin users can create, read, update and view all users, while registered users can only view their only user accounts:</p> <pre><code>class UserPolicy:\n    def __init__(self, user: User, user_obj: User) -&gt; None:\n        self.user = user\n        self.user_obj = user_obj\n\n    def create(self) -&gt; bool:\n        return self.user.is_admin:\n\n    def update(self) -&gt; bool:\n        return self.user.is_admin:\n\n    def get(self) -&gt; bool:\n        return (self.user.id == self.user.id) or self.user.is_admin\n\n    def scope(self) -&gt; list[User]:\n        if self.user.is_admin:\n            return self.user_obj.all()\n\n        return [self.user_obj.get(self.user.id)]\n</code></pre> <p>(Optional) And a custom exception class when current user not authorised to perform particular action(s) defined in the policy:</p> <pre><code>class PermissionRequired(BaseError):\n    def __init__(self):\n        super().__init__(403, \"Permission denied\")\n</code></pre> <p>To check if the current user is authorised to perform a particular <code>User</code> action - in this example update, in a routing/view:</p> <pre><code>from path.to.user_auth import UserAuth\nfrom path.to.user.model import User\n\n@app.post('/users/&lt;user_id&gt;')\ndef update(user_id):\n    user = User.get(user_id)\n    if UserAuth.authorize(user, \"update\"):\n        return user\n\n    return render_template(\"403.html\"), 403\n\n\n@app.post('/users')\ndef all():\n    user_records = UserAuth.authorised_scope(User)\n    return jsonify(user_records)\n</code></pre> <p>or in a service class:</p> <pre><code>from path.to.user_auth import UserAuth\nfrom path.to.user.model import User\n\nclass UsersService:\n    def update(self, user_id: str, name: str, email: str) -&gt; dict[str, str]:\n        user = User.get(user_id)\n        if not UserAuth.authorised_action(user, \"update\"):\n            raise PermissionRequired()\n\n        user.update(name=name, email=email)\n        return {\"message\": \"User successfully update\"}\n</code></pre> <p>The <code>UserAuth.authorize</code> takes two parameters:</p> <ul> <li>the model object (can be an instance or a class) you want to authorise the current user on</li> <li>the action or class method to authorise - in this case <code>update</code>. By default, if nothing passes as the parameter it will use the <code>request.method</code>, e.g. <code>post</code> in the example, and will require to define a <code>post</code> method in the policy:</li> </ul> <pre><code>class UserPolicy:\n    ...\n\n    def post(self) -&gt; bool:\n        return self.user.is_admin:\n\n    ...\n</code></pre>"},{"location":"Python/redis/","title":"Redis","text":""},{"location":"Python/redis/#cache-helper","title":"Cache helper","text":"<pre><code>from typing import TYPE_CHECKING, Any, Optional\n\nimport redis\n\nfrom app.config.config import config\n\nif TYPE_CHECKING:\n    from datetime import datetime\n\n\nclass RedisCache:\n    def __init__(self, decode_responses=True) -&gt; None:\n        host = config.redis_db.host\n        port = config.redis_db.port\n        self.redis_client = redis.StrictRedis(host=host, port=port, decode_responses=decode_responses)\n\n    def set(self, key: str, value: Any) -&gt; Optional[bool]:\n        self.redis_client.set(key, value)\n\n    def get(self, key: str) -&gt; Optional[bytes]:\n        return self.redis_client.get(key)\n\n    def set_key_expire_at(self, key: str, expire_at: \"datetime\") -&gt; None:\n        if self.redis_client.exists(key):\n            self.redis_client.expireat(key, expire_at)\n\n    def get_keys_by_pattern(self, keys_pattern: str) -&gt; list:\n        return [cache_key for cache_key in self.redis_client.scan_iter(keys_pattern)]\n\n    def delete_keys_by_pattern(self, keys_pattern: str) -&gt; list:\n        for cache_key in self.redis_client.scan_iter(keys_pattern):\n            self.redis_client.delete(cache_key)\n\n    def flushall(self) -&gt; None:\n        self.redis_client.flushall()\n\n    def exists(self, key) -&gt; int:\n        return self.redis_client.exists(key)\n\n    def store_and_set_expire(self, cache_key: str, data: str, expire_at: \"datetime\") -&gt; None:\n        self.set(cache_key, data)\n        self.set_key_expire_at(cache_key, expire_at)\n</code></pre>"},{"location":"Python/redis/#events-streaming-with-redis-stream","title":"Events streaming with Redis Stream","text":"<p>Redis documentation on Stream Use of consumer groups Redis-py</p>"},{"location":"Python/redis/#publisher","title":"Publisher","text":"<pre><code>import json\n\nfrom concurrent import futures\nfrom datetime import datetime\nfrom typing import Any\n\nimport redis\n\nfrom logger import Logger\n\n\n\nclass EventsPublisher:\n    def __init__(self, event_stream_name: str) -&gt; None:\n        self.logger = Logger().get_logger()\n        self.event_stream_name = event_stream_name\n                self._publisher = None\n\n        @properity\n        def publisher(self):\n            if self._publisher is None:\n                self._publisher = redis.Redis(\n                    host=config.event_publisher.host,\n                    port=config.event_publisher.port,\n                    db=config.event_publisher.database,\n                )\n            return self._publisher\n\n    def batch_add(self, payloads: list, id_key=\"obj_id\") -&gt; tuple[list[str], list[str]]:\n        successful_entries_ids = []\n        failed_obj_ids = []\n\n        with futures.ThreadPoolExecutor() as executor:\n            tasks = [executor.submit(self.add, payload, False, id_key) for payload in payloads]\n            for task in futures.as_completed(tasks):\n                try:\n                    entry_id, obj_id = task.result()\n                    if entry_id is not None:\n                        successful_entries_ids.append(entry_id)\n                    else:\n                        failed_obj_ids.append(obj_id)\n                except Exception:\n                    self.logger.exception(\"Error pushing data to stream\")\n\n        return successful_entries_ids, failed_obj_ids\n\n    def add(self, payload: Any, raise_error=True, id_key=\"id\") -&gt; tuple[str, str]:\n        obj_id = payload.get(id_key)\n        obj_type = payload.get(\"obj_type\")\n        action = payload.get(\"action\") # create / update etc\n\n        try:\n            json_payload = json.dumps(payload)\n            self.logger.info(\"Pushing %s data (%s) with ID %s to stream\", obj_type, action, obj_id)\n            entry_id = self.publisher.xadd(self.event_stream_name, {\"payload\": json_payload})\n            entry_id = entry_id.decode(\"utf-8\")\n            self.logger.info(\n                \"%s data (%s) with ID %s pushed - entry_id %s\", obj_type, action, obj_id, entry_id\n            )\n            return entry_id, obj_id\n        except (\n            TypeError,\n            redis.exceptions.ConnectionError,\n            redis.exceptions.TimeoutError,\n            redis.exceptions.RedisError,\n        ) as e:\n            msg = f\"Failed to push {obj_type} data ({action}) with ID {obj_id} to stream: {str(e)}\"\n            self.logger.info(msg)\n            if raise_error:\n                raise e\n            return None, obj_id\n\n    def delete(self, msg_id: str) -&gt; dict[str, int]:\n        status = self.publisher.xdel(self.event_stream_name, msg_id)\n        return {msg_id: status}\n\n    def get_list(self, msg_count=10):\n        event_msgs = self.publisher.xread({self.event_stream_name: \"0-0\"}, count=msg_count)\n        data = {}\n        if not event_msgs:\n            return data\n\n        for message in event_msgs[0][1]:\n            message_id = message[0].decode(\"utf-8\")\n\n            try:\n                message_body = message[1]\n                payload = message_body.get(b\"payload\")\n                payload = payload.decode(\"utf-8\") if payload else {}\n            except Exception:\n                payload = {}\n            data[message_id] = payload\n\n        return data\n</code></pre>"},{"location":"Python/redis/#consumer-one-consumer-no-additional-consumer-groups","title":"Consumer (One consumer - no additional consumer groups)","text":"<pre><code>import asyncio\nimport json\n\nfrom typing import Any\n\nfrom redis import asyncio as aioredis\n\nfrom path.to.config import config\n\n\n\nclass EventsConsumer:\n    def __init__(self, app) -&gt; None:\n        app.app_context().push()\n        self.logger = logger\n\n    async def handle_event(self, payload: dict) -&gt; None:\n            # process the event message body\n            await process_event_message(payload)\n\n\n    async def main(self) -&gt; Any:\n        consumer_config = config.event_consumer\n        consumer_host = consumer_config.stream_host\n        consumer_port = consumer_config.stream_port\n        consumer_database = consumer_config.stream_database\n        consumer_stream = consumer_config.stream\n\n        redis = await aioredis.from_url(\n            f\"redis://{consumer_host}:{consumer_port}\",\n            db=consumer_database,\n            encoding=\"utf8\",\n            decode_responses=True,\n        )\n        message_id = \"0-0\"\n\n        while True:\n            events = await redis.xread({consumer_stream: message_id})\n            if not events:\n                await asyncio.sleep(1)\n\n            for stream, message in events:\n                message_id = message[0][0]\n                payload = message[0][1][\"payload\"]\n                payload = json.loads(payload)\n\n                await self.handle_event(payload)\n                await redis.xdel(stream, message_id)\n</code></pre> <p>To start consumer as thread:</p> <pre><code>def start_event_consumer():\n    asyncio.run(EventsConsumer(app).main())\n\nworker = threading.Thread(target=start_event_consumer, args=[])\nworker.start()\n</code></pre>"},{"location":"Python/redis/#consumer-groups","title":"Consumer groups","text":"<p>Create consumer group(s) XGROUP CREATE:</p> <pre><code>redis_client.xgroup_create(name=stream_name, groupname=gname, id=0)\n</code></pre> <p>Read with consumer group(s) XREADGROUP:</p> <pre><code>redis_client.xreadgroup(groupname=group_1, consumername='consumer_a', streams={stream_key:'&gt;'})\n</code></pre> <p>Acknowledge messages XACK:</p> <pre><code>redis_client.xack(stream_name, groupname, message_id)\n</code></pre>"},{"location":"Python/sqlalchemy/","title":"Sqlalchemy","text":""},{"location":"Python/sqlalchemy/#intenum","title":"IntEnum","text":"<p>Storing the enum integer value to the database</p> <pre><code>from sqlalchemy import types\n\nclass IntEnum(types.TypeDecorator):\n    impl = Integer\n\n    def __init__(self, enumtype, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._enumtype = enumtype\n\n    def process_bind_param(self, value, dialect):\n        return value.value\n\n    def process_result_value(self, value, dialect):\n        return self._enumtype(value)\n</code></pre> <pre><code>class Role(enum.Enum):\n    REGISTERED = 1\n    MODERATOR = 2\n    ADMIN = 3\n\n\nclass User(db.Model):\n    __tablename__ = \"users\"\n\n    id: uuid.UUID = db.Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    name: str = db.Column(db.String(50), nullable=False)\n    email: str = db.Column(db.String(50), nullable=False)\n    role: int = db.Column(IntEnum(Role), nullable=False, default=1)\n</code></pre>"},{"location":"Python/sqlalchemy/#query-mixin","title":"Query mixin","text":"<pre><code>class QueryMixin:\n    @classmethod\n    def get(cls, id):\n        return cls.query.get(id)\n\n    @classmethod\n    def _filters(cls, kwargs):\n        return [getattr(cls, attr) == kwargs[attr] for attr in kwargs]\n\n    @classmethod\n    def find_by(cls, **kwargs):\n        filters = cls._filters(kwargs)\n        return db.session.execute(db.select(cls).where(*filters)).scalars().first()\n\n    @classmethod\n    def find_all(cls, **kwargs):\n        filters = cls._filters(kwargs)\n        return db.session.execute(db.select(cls).where(*filters)).scalars().all()\n\n    @classmethod\n    def find_all_in(cls, **kwargs):\n        filters = [getattr(cls, attr).in_(kwargs[attr]) for attr in kwargs]\n        return db.session.execute(db.select(cls).where(*filters)).scalars().all()\n\n    @classmethod\n    def find_all_not_in(cls, **kwargs):\n        filters = [getattr(cls, attr).not_in(kwargs[attr]) for attr in kwargs]\n        return db.session.execute(db.select(cls).where(*filters)).scalars().all()\n\n    @classmethod\n    def delete_if_exists(cls, **kwargs):\n        filters = cls._filters(kwargs)\n        cls.query.where(*filters).delete()\n        db.session.commit()\n\n    def delete(self):\n        db.session.delete(self)\n        db.session.commit()\n\n    def to_dict(self):\n        return {\n            column.name: getattr(self, column.name)\n            if not isinstance(getattr(self, column.name), (datetime, date))\n            else getattr(self, column.name).isoformat()\n            for column in self.__table__.columns\n        }\n</code></pre>"},{"location":"Python/sqlalchemy/#insertreturning","title":"Insert...returning","text":"<pre><code>    data = [\n        {\"name\": \"Sally\", \"email\": \"sally@user.email\"},\n        {\"name\": \"Jon\", \"email\": \"jon@user.email\"},\n        {\"name\": \"Ken\", \"email\": \"ken@user.email\"},\n        {\"name\": \"Jess\", \"email\": \"jess@user.email\"}\n    ]\n    employees = db.session.execute(insert(Employee).returning(Employee), data).all()\n</code></pre>"},{"location":"Python/sqlalchemy/#updatereturning","title":"Update...returning","text":"<pre><code>    employee_id = \"ec38f27b-79a2-4739-b96a-6bc2babcc2c9\"\n    update_params = {\"name\": \"Ken\", \"email\": \"ken@another.email\"}\n    update_stmt = update(Employee).where(Employee.id == employee_id).values(update_params).returning(Employee)\n    employees = db.session.execute(update_stmt).first()\n</code></pre>"},{"location":"Python/sqlalchemy/#update-multiple-records-where-the-where-conditons-is-different-for-each-record","title":"Update multiple records where the <code>WHERE</code> conditons is different for each record","text":"<pre><code>    data = [\n        {\"employee_id\": \"443bd75a-3baa-4b17-a391-af00af9e3325\", \"name\": \"Sally\", \"email\": \"sally@user.email\", \"new_department\": \"Marketing\"},\n        {\"employee_id\": \"52ca61aa-399d-4a0c-9482-6e0ec3ab4891\", \"name\": \"Jon\", \"email\": \"jon@user.email\", \"new_department\": \"IT\"},\n        {\"employee_id\": \"ec38f27b-79a2-4739-b96a-6bc2babcc2c9\", \"name\": \"Ken\", \"email\": \"ken@user.email\", \"new_department\": \"Finance\"},\n        {\"employee_id\": \"0d28cbbd-f073-4420-9f73-3dfc93c7696e\", \"name\": \"Jess\", \"email\": \"jess@user.email\", \"new_department\": \"Marketing\"},\n    ]\n\n    update_stmt = (\n        update(Employee)\n        .where(Employee.id == bindparam(\"employee_id\"))\n        .values(\n            {\"department_id\": select(Department.id).where(Department.name == bindparam(\"new_department\")).scalar_subquery()}\n        )\n    )\n    db.session.execute(update_stmt, data)\n</code></pre>"},{"location":"Python/sqlalchemy/#nested-transaction","title":"Nested transaction","text":"<pre><code>    db.session.begin_nested()\n</code></pre>"},{"location":"Python/testing/","title":"Testing","text":""},{"location":"Python/testing/#pytest","title":"Pytest","text":""},{"location":"Python/testing/#testing-endpoints","title":"Testing endpoints","text":"<pre><code>from path.to.app_factory import app # or create_app()\n\n@pytest.fixture\ndef app(request):\n    project_app = _app # or create_app() factory\n    ctx = project_app.app_context()\n    ctx.push()\n\n    yield project_app\n\n    ctx.pop()\n\n\n@pytest.fixture()\ndef test_client(app) -&gt; Flask:\n    yield app.test_client()\n</code></pre> <pre><code>class TestClsNamePage:\n    # Test get endpoint and assert content on page\n    def test_get_endpoint(self, test_client):\n        response = test_client.get(\"/\")\n        assert response.status_code == 200\n\n        page_content = response.data.decode(\"utf-8\")\n        assert \"Hello!\" in page_content\n\n    # Test endpoint with headers\n    def test_endponint_with_headers(self, test_client):\n        response = test_client.get(\"/\", headers={\"Auth-Token\": \"some-random-auth-token-values\"})\n        assert response.status_code == 200\n\n    # Test get endpoint and content on page after redirect\n    def test_get_endpoint(self, test_client):\n        response = test_client.get(\"/redirect_to_another_page\", follow_redirects=True)\n        assert response.status_code == 200\n\n        page_content = response.data.decode(\"utf-8\")\n        assert \"Redirected\" in page_content\n\n    # Test get endpoint with JSON response\n    def test_get_endpoint(self, test_client):\n        # assuming endpoint return {\"message\": \"Hello!\"}\n        response = test_client.get(\"/json_response\")\n        assert response.status_code == 200\n\n        data = json.loads(response.data)\n        assert data[\"message\"] == \"Hello!\"\n\n    # Test post endpoint - HTML form submission\n    def test_post_form_submit(self, test_client):\n        response = test_client.post(\"/users\", data={\"name\": \"user\", \"email\": \"user@test.com\"})\n        assert response.status_code == 200\n\n    # Test post endpoint - HTML form submission with multi select\n    def test_post_multi_select_form_submit(self, test_client):\n        # Remember to import ImmutableMultiDict: `from werkzeug.datastructures import ImmutableMultiDict`\n        form = ImmutableMultiDict(\n            [(\"user_id\", \"user_id_1\"), (\"user_id\", \"user_id_2\"), (\"user_id\", \"user_id_2\")]\n        )\n        response = test_client.post(\"/users/multi_select\", data=form)\n        assert response.status_code == 200\n\n    # Test post endpoint JSON params\n    def test_post_json_params(self, test_client):\n        response = test_client.post(\"/users\", json={\"name\": \"user\", \"email\": \"user@test.com\"})\n        assert response.status_code == 200\n</code></pre>"},{"location":"Python/testing/#parametrize-tests","title":"Parametrize tests","text":"<pre><code>import pytest\n\nclass TestClsName:\n    @pytest.mark.parametrize(\"param_1, param_2, param3, expected_result\", [\n        (test_1_param_1, test_1_param_2, test_1_param_3, test_1_expected_result),\n        (test_2_param_1, test_2_param_2, test_2_param_3, test_2_expected_result),\n        (test_3_param_1, test_3_param_2, test_3_param_3, test_3_expected_result),\n    ])\n    def test_some_test_with_parametrize(self, param_1, param_2, param3, expected_result):\n        result = method_to_test(param_1, param_2, param3)\n        assert result == expected_result\n</code></pre>"},{"location":"Python/testing/#mockpatching","title":"Mock/patching","text":"<p>Mock method to return specific value</p> <pre><code>class TestClsName:\n    def test_method_with_patch(self):\n        with patch(\"path.to.class.ClassName.method_name\") as mocked_method:\n            mocked_method.return_value = [1,2,3]\n</code></pre> <p>Mock method to raise exception</p> <pre><code>class TestClsName:\n    def test_method_with_mocked_exception(self):\n        with patch(\"path.to.class.ClassName.method_name\") as mocked_method:\n            mocked_method.side_effect = Exception(\"error\")\n</code></pre> <p>Mock response with JSON response, status_code</p> <pre><code>class MockResponse:\n    def __init__(self, json_data, status_code):\n        self.json_data = json_data\n        self.status_code = status_code\n\n    def json(self):\n        return self.json_data\n\nclass TestClsName:\n    def test_method_with_post_request(self):\n        with patch(\"app.model.ClsName.requests.post\") as mocked_request:\n            mocked_request.return_value=MockResponse({\"message\": \"ta-da\", \"status\": \"success\"}, 200)\n            result = method_with_post_request()\n            ...\n</code></pre> <p>Mock response with raise_for_status</p> <pre><code>class TestClsName:\n    def test_method_with_post_request_raise_for_status(self):\n        with patch(\"app.model.ClsName.requests.post\") as mocked_request:\n            mocked_status = Mock(status_code=500)\n            mocked_status.raise_for_status = Mock(side_effect=requests.exceptions.RequestException(\"Error\"))\n            mocked_request.return_value = mocked_status\n            result = method_with_post_request()\n            ...\n</code></pre>"},{"location":"Python/testing/#mock-aws-services-with-moto","title":"Mock AWS services with moto","text":"<p>For example, creating a S3 mock with moto</p> <pre><code>@pytest.fixture(scope=\"session\")\ndef aws_credentials():\n    \"\"\"Mocked AWS Credentials for moto.\"\"\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n    os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_DEFAULT_REGION\"] = \"eu-west-1\"\n\n\n@pytest.fixture(autouse=False)\ndef s3_client(aws_credentials):\n    with mock_s3():\n        conn = boto3.client(\"s3\", region_name=\"us-east-1\")\n        yield conn\n</code></pre> <p>And in tests, use <code>s3_client</code> as fixture</p> <pre><code>@pytest.fixture\ndef s3_bucket(s3_client):\n    s3_client.create_bucket(Bucket=bucket_name)\n    yield\n\nclass TestS3Service:\n    def test_s3_bucket_list_objects(self, s3_client, s3_bucket):\n        S3Service().list_objects()\n        ...\n</code></pre>"},{"location":"Python/testing/#monkeypatchingmocking-sqlalchemy-connection","title":"Monkeypatching/Mocking SQLAlchemy connection","text":"<p>For SQLAlchemy 2</p> <pre><code>@pytest.fixture(autouse=True, scope=\"function\")\ndef db(app, request):\n    # https://github.com/pallets-eco/flask-sqlalchemy/issues/1171\n    with app.app_context():\n        engines = _db.engines\n\n    engine_cleanup = []\n    for key, engine in engines.items():\n        connection = engine.connect()\n        transaction = connection.begin_nested()\n        engines[key] = connection\n        engine_cleanup.append((key, engine, connection, transaction))\n\n    try:\n        yield _db\n    finally:\n        for key, engine, connection, transaction in engine_cleanup:\n            transaction.rollback()\n            connection.close()\n            engines[key] = engine\n</code></pre> <p>For SQLAlchemy 1.4</p> <pre><code>@pytest.fixture\ndef app(request):\n    app = _app\n    with app.app_context():\n        yield app\n\n@pytest.fixture\ndef db(app, request, monkeypatch):\n    connection = _db.engine.connect()\n    transaction = connection.begin()\n\n    # https://github.com/pallets/flask-sqlalchemy/pull/249#issuecomment-628303481\n    monkeypatch.setattr(_db, \"get_engine\", lambda *args, **kwargs: connection)\n\n    try:\n        yield _db\n    finally:\n        _db.session.remove()\n        transaction.rollback()\n        connection.close()\n</code></pre> <p>And use <code>db</code> as fixture</p> <pre><code>@pytest.mark.usefixtures(\"db\")\nclass TestClsName:\n    ...\n</code></pre> <pre><code>def test_method_name(self, db):\n    ...\n</code></pre> <p>When testing database rollback use <code>db.session.begin_nested()</code> to begin a \"nested\" transaction/savepoint</p> <pre><code>class TestClsName:\n    def test_method_with_db_rollback(self, db):\n        create_test_objects()\n        db.session.begin_nested()\n        result = method_with_db_rollback()\n        assert result == expected_result\n</code></pre>"},{"location":"Python/testing/#assert-exception-raised","title":"Assert exception raised","text":"<pre><code>class TestUser:\n    def test_user_init_failed_on_missing_required_values(self):\n        with pytest.raises(TypeError) as error:\n            User()\n        assert str(error.value) == \"__init__() missing 2 required positional argument: 'name', 'email'\"\n</code></pre>"},{"location":"Python/testing/#assert-parameters-pass-to-methodmock-method","title":"Assert parameters pass to method/mock method","text":"<p>Passing the actual method to <code>side_effect</code> will call the actual method instead of \"mocked\"</p> <pre><code>    with patch.object(ClsName, \"method_name\", side_effect=ClsName().method_name) as mocked_method:\n        mocked_method.assert_called_with(\n            param_1=expected_param_1,\n            param_2=expected_param_2,\n            param_3=expected_param_3,\n        )\n</code></pre>"},{"location":"Python/testing/#mock-results-of-repeatedmultiple-calls-to-the-same-method","title":"Mock results of repeated/multiple calls to the same method","text":"<p>For example, calling a method in a loop:</p> <pre><code>def loop_me():\n    for i in range(3):\n        result = do_something()\n        print(result)\n</code></pre> <p>To mock the results of repeated <code>do_something</code> calls</p> <pre><code>    with patch(\"path.to.do_something\", side_effect=(4, 5, 6)):\n        loop_me()\n</code></pre>"},{"location":"Python/testing/#mock-property","title":"Mock property","text":"<pre><code>    class SomeClass:\n        @property\n        def property_x():\n            return something\n</code></pre> <pre><code>from mock import PropertyMock, patch\n\n    with patch(\"path.to.python.SomeClass.property_x\", new_callable=PropertyMock, return_value=some_value):\n        # do something\n</code></pre> <p>or</p> <pre><code>from mock import PropertyMock, patch\nfrom app.domains.obj_cls import ObjCls\n\n    with patch.object(ObjCls, \"attr_or_property_name\", new_callable=PropertyMock) as mocked:\n        mocked.return_value = some_value\n        # do something\n</code></pre>"},{"location":"Python/text_extraction/","title":"Text extraction","text":""},{"location":"Python/text_extraction/#text-extraction","title":"Text extraction","text":""},{"location":"Python/text_extraction/#pdf-extraction-using-pymupdf4","title":"PDF extraction using PyMuPDF4","text":"<p>PyMuPDF4 | PyMuPDF4LLM</p> <p>WIP</p>"},{"location":"Python/text_extraction/#pdf-extraction-using-docling","title":"PDF extraction using Docling","text":"<p>Docling</p> <p>WIP</p> <p>Adobe PDF Extract API</p>"}]}