{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>A bunch of notes on Python, Javascript, SQL etc - just to save myself from digging through all the repos to find the bits I need.</p>"},{"location":"SQL/","title":"SQL","text":""},{"location":"SQL/#postgres","title":"Postgres","text":""},{"location":"SQL/#tools-and-cheatsheet","title":"Tools and cheatsheet","text":"<p>Online playground: DB Fiddle GUI tool: TablePlus Postgres.app (Mac) Cheatsheet</p>"},{"location":"SQL/#enable-uuid-generation","title":"Enable uuid generation","text":"<pre><code>CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n</code></pre>"},{"location":"SQL/#create-table-with-foreign-key-constraintsreferences","title":"Create table with foreign key constraints/references","text":"<pre><code>CREATE TABLE departments (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nname VARCHAR(255) NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\nCREATE TABLE employees (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nemail VARCHAR(255) UNIQUE NOT NULL,\nfirst_name VARCHAR(255) NOT NULL,\nlast_name VARCHAR(255) NOT NULL,\ndepartment_id uuid NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL,\nFOREIGN KEY (department_id) REFERENCES departments(id) ON DELETE CASCADE\n);\n</code></pre> <p>or</p> <pre><code>CREATE TABLE departments (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nname VARCHAR(255) NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\nCREATE TABLE employees (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nemail VARCHAR(255) UNIQUE NOT NULL,\nfirst_name VARCHAR(255) NOT NULL,\nlast_name VARCHAR(255) NOT NULL,\ndepartment_id uuid NOT NULL REFERENCES departments(id) ON DELETE CASCADE,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\n</code></pre>"},{"location":"SQL/#insert-values-to-table","title":"Insert values to table","text":"<pre><code>INSERT INTO\ndepartments(name)\nVALUES\n('HR'),\n('Finance');\nINSERT INTO\nemployees(email, first_name, last_name, department_id)\nVALUES\n('mary@test_co.com', 'Mary', 'Smith', (SELECT id from departments WHERE name='Finance')),\n('dave@test_co.com', 'Dave', 'Cole', (SELECT id from departments WHERE name='Finance')),\n('jane@test_co.com', 'Jane', 'Hills', (SELECT id from departments WHERE name='Finance')),\n('john@test_co.com', 'John', 'Doe', (SELECT id from departments WHERE name='HR'));\n</code></pre>"},{"location":"SQL/#simple-audit-table","title":"Simple audit table","text":"<pre><code>CREATE OR REPLACE FUNCTION employees_audit_func()\nRETURNS TRIGGER\nAS $employees_audit$\nBEGIN\nif (TG_OP = 'UPDATE') THEN\nINSERT INTO employees_audit\nSELECT\nuuid_generate_v4(),\n'UPDATE',\nnow(),\nNEW.*;\nelsif (TG_OP = 'INSERT') THEN\nINSERT INTO employees_audit\nSELECT\nuuid_generate_v4(),\n'INSERT',\nnow(),\nNEW.*;\nelsif (TG_OP = 'DELETE') THEN\nINSERT INTO employees_audit\nSELECT\nuuid_generate_v4(),\n'DELETE',\nnow(),\nOLD.*;\nEND IF;\nRETURN NULL;\nEND;\n$employees_audit$\nLANGUAGE plpgsql;\nCREATE TRIGGER employees_audit_trigger\nAFTER INSERT OR UPDATE OR DELETE ON employees FOR EACH ROW\nEXECUTE PROCEDURE employees_audit_func();\n</code></pre>"},{"location":"SQL/#updated_at-timestamp-trigger","title":"updated_at timestamp trigger","text":"<pre><code>CREATE OR REPLACE FUNCTION trigger_set_timestamp()\nRETURNS TRIGGER AS $$\nBEGIN\nNEW.updated_at = NOW();\nRETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\nCREATE TRIGGER set_timestamp_employees\nBEFORE UPDATE ON employees\nFOR EACH ROW\nEXECUTE PROCEDURE trigger_set_timestamp();\n</code></pre>"},{"location":"SQL/#add-column-and-foreign-key-constraint-to-existing-table","title":"Add column and foreign key constraint to existing table","text":"<pre><code>ALTER TABLE employees\nADD COLUMN department_id uuid DEFAULT NULL, -- Set to NOT NULL once data is populated\nADD CONSTRAINT employee_id_department_id FOREIGN KEY (department_id) REFERENCES departments(id);\n</code></pre> <p>Set column to <code>NOT NULL</code></p> <pre><code>ALTER TABLE employees ALTER COLUMN department_id SET NOT NULL;\n</code></pre>"},{"location":"SQL/#alter-table-column-type-with-casting","title":"Alter table column type (with casting)","text":"<pre><code>CREATE TABLE employees (\n...\nemployment_start_year VARCHAR(4) NOT NULL\n...\n);\nALTER TABLE employees ALTER employment_start_year TYPE INT\nUSING employment_start_year::INTEGER;\n</code></pre>"},{"location":"SQL/#update-values-with-unnest","title":"Update values with unnest","text":"<pre><code>UPDATE\nemployees\nSET\nemail = data_table.email\nFROM (\nSELECT\nunnest(ARRAY ['Mary', 'John', 'Dawn']) AS first_name,\nunnest(ARRAY ['Smith', 'Doe', 'Carter']) AS last_name) AS data_table\nWHERE\nemployees.email = data_table.email;\n</code></pre>"},{"location":"SQL/#json-aggregation-and-functions","title":"JSON aggregation and functions","text":"<pre><code>SELECT\ndept.name AS department,\njsonb_agg(\njsonb_build_object (\n'employee_id', e.id,\n'email', e.email,\n'first_name', e.first_name,\n'last_name', e.last_name\n)\n) AS department_staff\nFROM\nemployees e\nJOIN departments dept ON e.department_id = dept.id\nGROUP BY\nd.name, d.id;\n</code></pre> <p>Returns:</p> department department_staff Finance [{\"email\": \"dave@test_co.com\", \"last_name\": \"Cole\", \"first_name\": \"Dave\", \"employee_id\": \"b26339e1-af22-4752-852e-cb51f342bb10\"}, {\"email\": \"jane@test_co.com\", \"last_name\": \"Hills\", \"first_name\": \"Jane\", \"employee_id\": \"710b5de9-f248-49d8-9846-57b31ed143b2\"}, {\"email\": \"mary@test_co.com\", \"last_name\": \"Smith\", \"first_name\": \"Mary\", \"employee_id\": \"e2044159-0576-4a3a-9fe5-9af24bf66102\"}] HR [{\"email\": \"john@test_co.com\", \"last_name\": \"Doe\", \"first_name\": \"John\", \"employee_id\": \"bcdadb45-970e-4d87-8b8a-fab1ccfeddd4\"}] <pre><code>SELECT\nd.name,\njsonb_object_agg(e.email, (concat_ws(' ', e.first_name, e.last_name))) AS department_staff\nFROM\nemployees e\nJOIN departments d ON e.department_id = d.id\nGROUP BY\nd.name,\nd.id;\n</code></pre> <p>Returns:</p> department department_staff Finance {\"dave@test_co.com\": \"Dave Cole\", \"jane@test_co.com\": \"Jane Hills\", \"mary@test_co.com\": \"Mary Smith\"} HR {\"john@test_co.com\": \"John Doe\"}"},{"location":"SQL/#get-week-number-of-a-date","title":"Get week number of a date","text":"<p>N.B. With last day of week is Saturady i.e. new week begins on Sunday</p> <pre><code>CREATE OR REPLACE FUNCTION get_week_number_for_date (date_input date) RETURNS int LANGUAGE plpgsql AS $$ BEGIN RETURN (\n(\n$1 - DATE_TRUNC('year', $1)::date\n) + DATE_PART('isodow', DATE_TRUNC('year', $1))\n)::int / 7 + CASE\nWHEN DATE_PART('isodow', DATE_TRUNC('year', $1)) = 7 THEN 0\nELSE 1\nEND;\nEND;\n$$;\nget_week_number_for_date('2020-01-01');\n</code></pre>"},{"location":"SQL/#count-business-days-between-2-dates","title":"Count business days between 2 dates","text":"<pre><code>CREATE OR REPLACE FUNCTION business_days_count (from_date date, to_date date)\nRETURNS int\nLANGUAGE plpgsql\nAS $$\nBEGIN\nRETURN (SELECT\ncount(d::date) AS d\nFROM\ngenerate_series(from_date, to_date, '1 day'::interval) d\nWHERE\nextract('dow' FROM d)\nNOT in(0, 6));\nEND;\n$$;\n</code></pre>"},{"location":"SQL/#generate_series","title":"GENERATE_SERIES","text":"<p><code>GENERATE_SERIES</code> is pretty handy when creating time-series dataset</p> <pre><code>SELECT * FROM GENERATE_SERIES(2019, 2021, 1) AS \"year\", GENERATE_SERIES(1, 12, 1) AS \"month\";\n</code></pre> <p>Will generate a year-month table</p> year month 2019 1 2019 2 ... ... 2019 12 2020 1 2020 2 ... ... 2020 12 2021 1 2021 2 ... ... 2021 12 <p>...and can also produce a range of dates/time</p> <pre><code>SELECT * FROM generate_series('2022-01-01','2022-01-02', INTERVAL '1 hour');\n</code></pre> generate_series 2022-01-01T00:00:00.000Z 2022-01-01T01:00:00.000Z 2022-01-01T02:00:00.000Z 2022-01-01T03:00:00.000Z 2022-01-01T04:00:00.000Z 2022-01-01T05:00:00.000Z 2022-01-01T06:00:00.000Z 2022-01-01T07:00:00.000Z 2022-01-01T08:00:00.000Z 2022-01-01T09:00:00.000Z 2022-01-01T10:00:00.000Z 2022-01-01T11:00:00.000Z 2022-01-01T12:00:00.000Z 2022-01-01T13:00:00.000Z 2022-01-01T14:00:00.000Z 2022-01-01T15:00:00.000Z 2022-01-01T16:00:00.000Z 2022-01-01T17:00:00.000Z 2022-01-01T18:00:00.000Z 2022-01-01T19:00:00.000Z 2022-01-01T20:00:00.000Z 2022-01-01T21:00:00.000Z 2022-01-01T22:00:00.000Z 2022-01-01T23:00:00.000Z 2022-01-02T00:00:00.000Z <p>... and add some random generated data</p> <pre><code>SELECT random() as rand_figures, *\nFROM generate_series('2022-01-01','2022-01-02', INTERVAL '1 hour');\n</code></pre> rand_figures generate_series 0.203633273951709 2022-01-01T00:00:00.000Z 0.571097886189818 2022-01-01T01:00:00.000Z 0.629665858577937 2022-01-01T02:00:00.000Z 0.0612306422553957 2022-01-01T03:00:00.000Z 0.431237444281578 2022-01-01T04:00:00.000Z 0.229508123826236 2022-01-01T05:00:00.000Z 0.867487183306366 2022-01-01T06:00:00.000Z 0.758365222252905 2022-01-01T07:00:00.000Z 0.155569355469197 2022-01-01T08:00:00.000Z 0.786357307806611 2022-01-01T09:00:00.000Z 0.284404154401273 2022-01-01T10:00:00.000Z 0.367461221758276 2022-01-01T11:00:00.000Z 0.754724379163235 2022-01-01T12:00:00.000Z 0.0396546637639403 2022-01-01T13:00:00.000Z 0.276610609609634 2022-01-01T14:00:00.000Z 0.96564608765766 2022-01-01T15:00:00.000Z 0.127415937371552 2022-01-01T16:00:00.000Z 0.110610570758581 2022-01-01T17:00:00.000Z 0.764237959869206 2022-01-01T18:00:00.000Z 0.24844411527738 2022-01-01T19:00:00.000Z 0.0547867906279862 2022-01-01T20:00:00.000Z 0.977096977643669 2022-01-01T21:00:00.000Z 0.677903080359101 2022-01-01T22:00:00.000Z 0.173856796696782 2022-01-01T23:00:00.000Z 0.896873883903027 2022-01-02T00:00:00.000Z"},{"location":"SQL/#covert-datetimedatetimestamp-to-string","title":"Covert datetime/date/timestamp to string","text":"<pre><code>SELECT TO_CHAR(TIMESTAMP '2023-01-01 05:00:00', 'YYYY-MM-DD');\n</code></pre>"},{"location":"airflow/","title":"Airflow","text":""},{"location":"airflow/#run-with-docker","title":"Run with Docker","text":"<p>Airflow Installlation with docker images</p>"},{"location":"airflow/#docker-compose-file-w-postgres-mongodb","title":"docker compose file (w/ postgres + mongodb)","text":"<pre><code># Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\n#\n# WARNING: This configuration is for local development. Do not use it in a production deployment.\n#\n# This configuration supports basic configuration using environment variables or an .env file\n# The following variables are supported:\n#\n# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\n#                                Default: apache/airflow:2.5.3\n# AIRFLOW_UID                  - User ID in Airflow containers\n#                                Default: 50000\n# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.\n#                                Default: .\n# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\n#\n# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\n#                                Default: airflow\n# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\n#                                Default: airflow\n# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\n#                                Use this option ONLY for quick checks. Installing requirements at container\n#                                startup is done EVERY TIME the service is started.\n#                                A better way is to build a custom image or extend the official image\n#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.\n#                                Default: ''\n#\n# Feel free to modify this file to suit your needs.\n---\nversion: \"3.8\"\nx-airflow-common: &amp;airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.3}\n  build: .\n  environment: &amp;airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    # For backward compatibility, with Airflow &lt;2.3\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: \"\"\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \"true\"\n    AIRFLOW__CORE__LOAD_EXAMPLES: \"true\"\n    AIRFLOW__API__AUTH_BACKENDS: \"airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session\"\n    # yamllint disable rule:line-length\n    # Use simple http server on scheduler for health checks\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n    # yamllint enable rule:line-length\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: \"true\"\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on: &amp;airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n    mongo:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n      TEST_APP_USER: test_app_user\n      TEST_APP_PASSWORD: test_app\n      TEST_APP_DB: test_app\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n      - ./init_scripts/sql/init_db.sh:/docker-entrypoint-initdb.d/init_db.sh\n      - ./init_scripts/sql:/sql\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 10s\n      retries: 5\n      start_period: 5s\n    restart: always\n\n  mongo:\n    image: mongo:7.0.0\n    restart: always\n    healthcheck:\n      test: [\"CMD\", \"mongosh\", \"--eval\", \"db.adminCommand('ping')\"]\n      interval: 5s\n      timeout: 5s\n      retries: 3\n      start_period: 5s\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: mongo_test_db\n      MONGO_INITDB_ROOT_PASSWORD: mongo_test_db\n    ports:\n      - \"27017:27017\"\n\n  redis:\n    image: redis:latest\n    expose:\n      - 6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n    restart: always\n\n  airflow-webserver:\n    &lt;&lt;: *airflow-common\n    command: webserver\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-scheduler:\n    &lt;&lt;: *airflow-common\n    command: scheduler\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-worker:\n    &lt;&lt;: *airflow-common\n    command: celery worker\n    healthcheck:\n      test:\n        - \"CMD-SHELL\"\n        - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    environment:\n      &lt;&lt;: *airflow-common-env\n      # Required to handle warm shutdown of the celery workers properly\n      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n      DUMB_INIT_SETSID: \"0\"\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-triggerer:\n    &lt;&lt;: *airflow-common\n    command: triggerer\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"',\n        ]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-init:\n    &lt;&lt;: *airflow-common\n    entrypoint: /bin/bash\n    # yamllint disable rule:line-length\n    command:\n      - -c\n      - |\n        function ver() {\n          printf \"%04d%04d%04d%04d\" $${1//./ }\n        }\n        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO &amp;&amp; gosu airflow airflow version)\n        airflow_version_comparable=$$(ver $${airflow_version})\n        min_airflow_version=2.2.0\n        min_airflow_version_comparable=$$(ver $${min_airflow_version})\n        if (( airflow_version_comparable &lt; min_airflow_version_comparable )); then\n          echo\n          echo -e \"\\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\\e[0m\"\n          echo \"The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!\"\n          echo\n          exit 1\n        fi\n        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n          echo\n        fi\n        one_meg=1048576\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n        warning_resources=\"false\"\n        if (( mem_available &lt; 4000 )) ; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( cpus_available &lt; 2 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( disk_available &lt; one_meg * 10 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if [[ $${warning_resources} == \"true\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n          echo \"Please follow the instructions to increase amount of resources available:\"\n          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n          echo\n        fi\n        mkdir -p /sources/logs /sources/dags /sources/plugins\n        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}\n        exec /entrypoint airflow version\n    # yamllint enable rule:line-length\n    environment:\n      &lt;&lt;: *airflow-common-env\n      _AIRFLOW_DB_UPGRADE: \"true\"\n      _AIRFLOW_WWW_USER_CREATE: \"true\"\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      _PIP_ADDITIONAL_REQUIREMENTS: \"\"\n    user: \"0:0\"\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}:/sources\n\n  airflow-cli:\n    &lt;&lt;: *airflow-common\n    profiles:\n      - debug\n    environment:\n      &lt;&lt;: *airflow-common-env\n      CONNECTION_CHECK_MAX_COUNT: \"0\"\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n    command:\n      - bash\n      - -c\n      - airflow\n\n  # You can enable flower by adding \"--profile flower\" option e.g. docker-compose --profile flower up\n  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n  # See: https://docs.docker.com/compose/profiles/\n  flower:\n    &lt;&lt;: *airflow-common\n    command: celery flower\n    profiles:\n      - flower\n    ports:\n      - \"5555:5555\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\nvolumes:\n  postgres-db-volume:\n</code></pre>"},{"location":"airflow/#dockerfile-running-airflow-with-additional-modulesrequirementstxt","title":"Dockerfile (Running airflow with additional modules/requirements.txt)","text":"<pre><code>FROM apache/airflow:2.7.0-python3.11\n\nUSER root\nRUN apt-get update \\\n    &amp;&amp; apt-get install -y --no-install-recommends \\\n    vim \\\n    &amp;&amp; apt-get autoremove -yqq --purge \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\nUSER airflow\n\nCOPY requirements.txt /\nRUN pip install --no-cache-dir \"apache-airflow==${AIRFLOW_VERSION}\" -r /requirements.txt\n</code></pre>"},{"location":"airflow/#populatesql-migration-on-postgres-docker-start","title":"Populate/SQL migration on Postgres docker start","text":"<pre><code>      - ./init_scripts/sql/init_db.sh:/docker-entrypoint-initdb.d/init_db.sh\n      - ./init_scripts/sql:/sql\n</code></pre> <p>Add SQL scripts to the <code>/init_scripts/sql</code> directory e.g.:</p> <pre><code>-- ./init_scripts/sql/001-init_tables.sql\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\nCREATE TABLE IF NOT EXISTS departments (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nname VARCHAR(255) NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\nCREATE TABLE IF NOT EXISTS employees (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nemail VARCHAR(255) UNIQUE NOT NULL,\nfirst_name VARCHAR(255) NOT NULL,\nlast_name VARCHAR(255) NOT NULL,\ndepartment_id uuid NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL,\nFOREIGN KEY (department_id) REFERENCES departments(id) ON DELETE CASCADE\n);\nINSERT INTO\ndepartments(name)\nVALUES\n('HR'), ('Finance');\nINSERT INTO\nemployees(email, first_name, last_name, department_id)\nVALUES\n('mary@test_co.com', 'Mary', 'Smith', (SELECT id from departments WHERE name='Finance')),\n('dave@test_co.com', 'Dave', 'Cole', (SELECT id from departments WHERE name='Finance')),\n('jane@test_co.com', 'Jane', 'Hills', (SELECT id from departments WHERE name='Finance')),\n('john@test_co.com', 'John', 'Doe', (SELECT id from departments WHERE name='HR'));\n</code></pre> <p>Add DB migrations files or SQL commands to <code>init_db.sh</code></p> <pre><code>#!/bin/bash\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" -d \"$POSTGRES_DB\"  &lt;&lt;-EOSQL\n  CREATE USER $TEST_APP_USER WITH PASSWORD '$TEST_APP_PASSWORD';\n    CREATE DATABASE $TEST_APP_DB;\n    GRANT ALL PRIVILEGES ON DATABASE $TEST_APP_DB TO $TEST_APP_USER;\nEOSQL\npsql -U \"$TEST_APP_USER\" -d \"$TEST_APP_DB\" -a -f /sql/001-init_tables.sql\n</code></pre>"},{"location":"docker/","title":"Docker","text":"<p>Docker cheat sheet</p>"},{"location":"docker/#create-and-start-containers","title":"Create and start containers","text":"<pre><code>docker-compose up\n</code></pre> <p>To run in background</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"docker/#stop-containers","title":"Stop containers","text":"<pre><code>docker-compose down\n</code></pre> <p>...and remove stopped containers</p> <pre><code>docker-compose down --remove-orphans\ndocker compose down --volumes --remove-orphans\n</code></pre>"},{"location":"docker/#docker-entrypointsh","title":"docker-entrypoint.sh","text":"<p> For Linux  If you want to access host from a docker container, you can find out more on how to do it here</p>"},{"location":"docker/#postgres","title":"Postgres","text":"<p> <code>docker-compose.yml</code> example </p> <pre><code>version: \"3.9\"\nservices:\n  postgres:\n    image: postgres:15.2\n    restart: always\n    environment:\n      POSTGRES_USER: db_user\n      POSTGRES_PASSWORD: db_password\n      POSTGRES_DB: db_name\n    ports:\n      - \"5432:5432\"\n</code></pre> <p>Postgres in container to run with different port number:</p> <pre><code>ports:\n  - \"5434:5434\"\ncommand: -p 5434\n</code></pre> <p>Healthcheck on postgres container:</p> <pre><code>postgres:\n    ...\n  healthcheck:\n    test: [\"CMD-SHELL\", \"pg_isready -U db_user -d db_name -p 5432\"]\ninterval: 10s\n    timeout: 5s\n    retries: 5\n</code></pre>"},{"location":"docker/#flask","title":"Flask","text":"<p><code>Dockerfile</code></p> <pre><code>FROM python:3.11.3-slim\nRUN apt\u2212get \u2212y update\nRUN apt\u2212get install \u2212y pip3 build\u2212essential\n\nWORKDIR /app\nCOPY ./requirements.txt /app\nCOPY . ./app\nRUN pip install -r requirements.txt\n\nEXPOSE 5000\n# where the flask app initiate\nENV FLASK_APP=/app/my_app/app.py\n# or `production`, `uat` etc\nENV FLASK_ENV=development\nENV FLASK_DEBUG=1\nENV FLASK_RUN_PORT=5000\n# if not using `docker-entrypoint` mentioned earlier\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\n# or with gunicorn\n# CMD [\"gunicorn\", \"-b\", \":5000\", \"my_app.app:app\"]\n</code></pre> <p>add <code>python3 -m flask run</code> to the end of <code>docker-entrypoint.sh</code> if using <code>ENTRYPOINT</code></p> <p>Built the Docker image</p> <pre><code>docker build -t my-flask-app .\n</code></pre> <p>and run the container:</p> <pre><code>docker run -p 5000:5000 my-flask-app\n</code></pre>"},{"location":"docker/#with-docker-compose","title":"with docker compose","text":"<p>Flask + Postgres + Celery w/ Redis (Worker + Beat + Flower)</p> <pre><code>version: \"3.9\"\nservices:\n  flask_app:\n    build:\n      context: .\n    environment:\n      FLASK_ENV: development\n      FLASK_APP: /app/my_app/app.py\n      FLASK_DEBUG: 1\nFLASK_RUN_PORT: 5000\nentrypoint: ./dev-entrypoint.sh\n    volumes:\n      - .:/app\n    ports:\n      - 5000:5000\n    links:\n      - postgres\n      - redis\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n\npostgres:\n    image: postgres:15.2\n    environment:\n      POSTGRES_USER: db_user\n      POSTGRES_PASSWORD: db_password\n      POSTGRES_DB: db_name\n    ports:\n      - \"5432:5432\"\nhealthcheck:\n      test:[\"CMD-SHELL\", \"pg_isready -U db_user -d db_name\"]\ninterval: 5s\n      timeout: 5s\n      retries: 5\nredis:\n    image: redis:latest\n    ports:\n      - \"6379:6379\"\nhealthcheck:\n      test: [\"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\"]\ncelery-worker:\n    build:\n      context: .\n    hostname: worker\n    entrypoint: celery\n    command: -A my_app.celery worker --loglevel=info  # change `my_app.celery` to where celery is init\n    volumes:\n      - .:/app\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n\ncelery-beat:\n    build:\n      context: .\n    hostname: beat\n    entrypoint: celery\n    command: -A my_app.celery beat --loglevel=info\n    volumes:\n      - .:/app\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n\ncelery-flower:\n    build:\n      context: .\n    hostname: flower\n    entrypoint: celery\n    command: -A my_app.celery flower --loglevel=info\n    volumes:\n      - .:/app\n        ports:\n      - 5555:5555\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n</code></pre>"},{"location":"git/","title":"Git","text":""},{"location":"git/#multi-git-accounts-access","title":"Multi git accounts access","text":"<p>If you work on different projects for different companies, or just want to separate company work from your own personal projects. You can use <code>includeIf</code> to tell git which identity/account to use depending on the directory path - for example, projects under <code>~/work/</code> folder are linked to the work account, <code>~/personal/</code> ones are linked to personal account</p> <p>First, in <code>~/.gitconfig</code> we do</p> <pre><code>[includeIf \"gitdir:~/work/\"]\n    path = .gitconfig-work\n[includeIf \"gitdir:~/personal/\"]\n    path = .gitconfig-personal\n</code></pre> <p>then edit/create (if haven't got one) <code>~/.gitconfig-personal</code> and add the git config of your personal account</p> <pre><code>[user]\nname = personalAccName\nemail = personal-acc@email-address-here.com\n</code></pre> <p>do the same for <code>~/.gitconfig-work</code> but with your work account info</p> <pre><code>[user]\nname = workAccName\nemail = work-acc@email-address-here.com\n</code></pre>"},{"location":"git/#add-remote-repo","title":"Add remote repo","text":"<pre><code>git remote add origin https://github.com/OWNER/REPOSITORY.git\n</code></pre>"},{"location":"git/#list-existing-remotes","title":"List existing remotes","text":"<pre><code>git remote -v\n</code></pre>"},{"location":"git/#change-remote-repos-url","title":"Change remote repo's URL","text":"<pre><code>git remote set-url origin https://github.com/OWNER/REPOSITORY.git\n</code></pre>"},{"location":"git/#rebse-branch-to-another","title":"Rebse branch to another","text":"<pre><code>git rebase origin/branch_name\n</code></pre>"},{"location":"git/#squash-amend-reword-commits-with-interactive-rebase-git-rebase-i","title":"Squash, amend, reword commits with interactive rebase (<code>git rebase -i</code>)","text":""},{"location":"git/#reword-commit-message","title":"Reword commit message","text":"<p>For example, I want to reword one of the last 3 commits</p> <pre><code>git rebase -i HEAD~3\n</code></pre> <p>will output the following</p> <pre><code>pick c2a4d95 reword me please\npick 2a4e409 update SQL query to insert new employee records\npick 08c8836 Refactor and clean up employee class\n\n# Rebase 8d8a4b3..08c8836 onto 8d8a4b3 (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n</code></pre> <p>To reword commit message <code>c2a4d95</code>, we replace <code>pick</code> with <code>reword</code>/<code>r</code> for that commit, then save and exit</p> <pre><code>reword c2a4d95 reword me please\npick 2a4e409 update SQL query to insert new employee records\npick 08c8836 Refactor and clean up employee class\n</code></pre> <p>we can now amend the commit message on the next \"screen\" - update the message then save and exit again:</p> <pre><code>Update documentation\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n...\n</code></pre> <p>It should output a message similar to the one below if rebased successfully:</p> <pre><code>[detached HEAD 7a8daf8] Update documentation\n Date: Sat Apr 22 10:01:48 2023 +0100\n 1 file changed, 71 insertions(+)\n create mode 100644 docs/docker.md\nSuccessfully rebased and updated refs/heads/rebase-test.\n</code></pre>"},{"location":"git/#squash-commits","title":"Squash commits","text":"<p>Using the same example from <code>reword</code> above, we now want to squash the three commits into one and amend the first commit message again:</p> <pre><code>reword 7a8daf8 Update documentation\nfixup 96dce5b update SQL query to insert new employee records\nfixup 467b4cc Refactor and clean up employee class\n</code></pre> <p>Here we use <code>fixup</code>/<code>f</code> instead of <code>squash</code>/<code>s</code> as we are discarding all the other commit messages except the top one.</p> <p>Again we should get <code>Successfully rebased and updated ...</code> if rebased successfully.</p>"},{"location":"localstack/","title":"Localstack","text":"<p>LocalStack provides a fully functional AWS cloud stack that can be run locally for development and testing purposes without using the actual AWS cloud services.</p>"},{"location":"localstack/#install-localstack-and-run","title":"Install Localstack and run","text":"<pre><code>pip install localstack\nlocalstack start -d\n</code></pre> <p>To get the status of each service</p> <pre><code>localstack status services\n</code></pre>"},{"location":"localstack/#run-via-docker-compose","title":"Run via Docker compose","text":"<p> <code>docker-compose.yml</code> for Localstack S3, SQS and DynamoDB </p> <pre><code>version: \"3.9\"\nservices:\n  localstack:\n    container_name: \"${LOCALSTACK_DOCKER_NAME-localstack_main}\"\nimage: localstack/localstack:latest\n    ports:\n      - \"4566:4566\"\n- \"4510-4559:4510-4559\"\nenvironment:\n      - DEBUG=1\n- DOCKER_HOST=unix:///var/run/docker.sock\n            - SERVICES=s3,sqs,dynamodb\n    volumes:\n      - \"${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack\"\n- \"/var/run/docker.sock:/var/run/docker.sock\"\n- \"./data:/tmp/localstack\"\nhealthcheck:\n            test:\n        - CMD\n        - bash\n        - -c\n        - awslocal dynamodb list-tables\n          &amp;&amp; awslocal s3 ls\n          &amp;&amp; awslocal sqs list-queues\n            interval: 10s\n            timeout: 5s\n            retries: 5\n</code></pre>"},{"location":"localstack/#populating-data-into-localstack-s3-on-docker-compose-up","title":"Populating data into localstack S3 on <code>docker compose up</code>","text":"<p>Add these two lines to the <code>volumes</code> in the <code>docker-compose.yml</code> (assuming the data/files we want to upload to loaclstack S3 are in <code>s3_data</code> folder)</p> <pre><code>      - \"./.localstack:/etc/localstack/init/ready.d\"\n      - \"./s3_data:/s3_data\" # location of files used to pre-populate the Localstack S3 bucket\n</code></pre> <p>Then create <code>.localstack</code> directory, and inside this folder create a shell script (e.g. <code>create_and_populate_bucket.sh</code>) with commands to create and upload/synchronise the localstack S3 bucket with local data:</p> <pre><code>awslocal s3api create-bucket --bucket mock-bucket\nawslocal s3api put-bucket-policy --bucket mock-bucket --policy \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Sid\\\":\\\"PublicReadGetObject\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":\\\"*\\\",\\\"Action\\\":\\\"s3:GetObject\\\",\\\"Resource\\\":\\\"arn:aws:s3:::mock-bucket/*\\\"}]}\"\nawslocal s3 sync /s3_data s3://mock-bucket\n</code></pre> <p>The bucket policy is basically the inline version of this:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::mock-bucket/*\"\n        },\n    ]\n}\n</code></pre>"},{"location":"Javascript/datatable/","title":"Datatable","text":"<pre><code>const reloadDatableData = () =&gt; {\nconst dataUrl = `/data-api-endpoint`;\nfetch(dataUrl)\n.then((response) =&gt; response.json())\n.then((data) =&gt; {\nconst datatable = $(\"#datatable-id\").DataTable();\ndatatable.clear().rows.add(data).draw();\n// in case of table width not working properly (responsive)\nsetTimeout(function () {\n$(\"#datatable-id\").DataTable().columns.adjust().draw();\n}, 100);\n});\n};\n</code></pre>"},{"location":"Javascript/datatable/#rezise-tables-to-fit-container-in-bootstrap-v5-tabs","title":"Rezise table(s) to fit container in bootstrap (v5) tabs","text":"<pre><code>const initTabsEvts = () =&gt; {\nconst tabEl = document.querySelectorAll('a[data-bs-toggle=\"tab\"]');\ntabEl.forEach((ele) =&gt; {\nele.addEventListener(\"shown.bs.tab\", function (event) {\nconst datatables = document.querySelector(\".tab-pane.active .datatable\");\ndatatables.forEach((datatable) =&gt; {\nsetTimeout(function () {\n$(datatable).DataTable().columns.adjust().draw();\n}, 50);\n});\n});\n});\n};\n</code></pre>"},{"location":"Javascript/html-to-image/","title":"Html to image","text":""},{"location":"Javascript/html-to-image/#generate-an-image-from-a-dom-node-and-exportdownload","title":"Generate an image from a DOM node (and export/download)","text":"<p>Follow html-to-image GitHub repo for install instruction or get min.js from cdnjs</p> <p>E.g. trying to generate and download an image of <code>div#screenshot-me</code>:</p> <pre><code>&lt;div id=\"screenshot-me\"&gt;\n&lt;div&gt;\n&lt;img src=\"https://picsum.photos/id/29/536/354\" /&gt;\n&lt;/div&gt;\n&lt;p&gt;\n    Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo\n    ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis\n    parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec,\n    pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec\n    pede justo, fringilla vel, aliquet nec, vulputate eget, arcu.\n  &lt;/p&gt;\n&lt;p&gt;\n    In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam\n    dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus.\n    Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo\n    ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem\n    ante, dapibus in, viverra quis, feugiat a, tellus.\n  &lt;/p&gt;\n&lt;p&gt;\n    Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean\n    imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies\n    nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum\n    rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum.\n    Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem.\n  &lt;/p&gt;\n&lt;/div&gt;\n&lt;button\nid=\"screenshot-btn\"\nonclick=\"takeCardScreenShot('screenshot-me', 'screenshot.png')\"\n&gt;\n  Take a screenshot!\n&lt;/button&gt;\n</code></pre> <p>JavaSrcipt to generate the image and trigger download (html-to-image v 1.11.11)</p> <pre><code>const takeScreenShot = (eleId, filename) =&gt; {\nhtmlToImage\n.toCanvas(document.getElementById(eleId), {\nquality: 1,\nbackgroundColor: \"#FFFFFF\",\n})\n.then((canvas) =&gt; {\nimgSaveAs(canvas.toDataURL(\"image/png\"), `${filename}.png`);\n})\n.catch((error) =&gt; {\nconsole.log(error);\n});\n};\nconst imgSaveAs = (uri, filename) =&gt; {\nconst link = document.createElement(\"a\");\nif (typeof link.download === \"string\") {\nlink.href = uri;\nlink.download = filename;\ndocument.body.appendChild(link);\nlink.click();\ndocument.body.removeChild(link);\n} else {\nwindow.open(uri);\n}\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/","title":"Javascript notes","text":""},{"location":"Javascript/javascript%20notes/#copy-to-clipboard","title":"Copy to clipboard","text":"<pre><code>const copyToClipboard = (txt) =&gt; {\ntry {\nnavigator.clipboard.writeText(url);\n} catch (e) {\nunsecuredCopyToClipboard(txt);\n}\nshowToastNotification(\"Copied to clipboard\", \"success\");\n};\nconst unsecuredCopyToClipboard = (text) =&gt; {\nconst ta = document.createElement(\"textarea\");\nta.value = text;\ndocument.body.appendChild(ta);\nta.focus();\nta.select();\ntry {\ndocument.execCommand(\"copy\");\n} catch (err) {\nconsole.error(\"Unable to copy to clipboard\", err);\n}\ndocument.body.removeChild(ta);\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#fetch","title":"fetch","text":"<pre><code>const fetchExample = (url) =&gt; {\nfetch(url, {\nmethod: \"POST\",\nheaders: {\nAccept: \"application/json\",\n\"Content-Type\": \"application/json\",\n},\nbody: JSON.stringify({ ... }),\n})\n.then((res) =&gt; {\nif (res.ok) {\nreturn res.json();\n}\nreturn Promise.reject(res);\n})\n.then((body) =&gt; {\n...\n})\n.catch((error) =&gt; {\n...\n})\n.finally(() =&gt; {\n...\n});\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#axios","title":"Axios","text":"<p>(Axios)[https://axios-http.com/]</p>"},{"location":"Javascript/plotly/","title":"Plotly","text":""},{"location":"Javascript/plotly/#plotly-js","title":"Plotly JS","text":"<p>Placeholder for chart in HTML:</p> <pre><code>&lt;div id=\"chart-id-here\" class=\"plotly-charts\"&gt;&lt;/div&gt;\ninitChart(\"chart-id-here\");\n</code></pre> <p>Javascript to fetch chart data from API endpoint</p> <pre><code>const initChart = (eleId) =&gt; {\nfetch(\"/api-endpoint-for-chart-data\")\n.then((response) =&gt; response.json())\n.then((data) =&gt; {\nconst chartData = data.chart_data;\nconst plotConfigData = data.config_data;\nplotlyChart(eleId, chartData, null, plotConfigData);\n});\n};\n</code></pre> <pre><code>const plotlyChartLayout = (title, opts) =&gt; {\nlet layout = $.extend(\n{\ntitle: {\ntext: title,\nfont: {\nsize: 12,\n},\n},\nxaxis: {\ntickfont: {\nsize: 11,\n},\n},\nyaxis: {\nautomargin: true,\ntickfont: {\nsize: 11,\n},\n},\nshowlegend: true,\nlegend: {\nfont: {\nsize: 10,\n},\n},\nbargap: 0.2,\nfont: { color: \"#4e4e4e\" },\n},\nopts\n);\nreturn layout;\n};\nconst plotlyChart = (\ntargetEle,\ndata,\ntitle,\nopts = null,\nconfigOpts = null,\nrefreshData = false\n) =&gt; {\nconst layout = plotlyChartLayout(title, opts);\nconst imgFilename = opts.filename ? opts.filename : \"plot-image\";\nconst config = $.extend(\n{\nresponsive: true,\ndisplayModeBar: true,\n// to hide the buttons/chart functions we don't want\nmodeBarButtonsToRemove: [\n\"zoom2d\",\n\"pan2d\",\n\"select2d\",\n\"lasso2d\",\n\"zoomIn2d\",\n\"zoomOut2d\",\n\"autoScale2d\",\n\"resetScale2d\",\n],\ntoImageButtonOptions: {\nformat: \"png\",\nfilename: imgFilename,\nscale: 2,\n},\ndisplaylogo: false,\n},\nconfigOpts\n);\nif (refreshData) {\nPlotly.react(targetEle, data, layout, config);\n} else {\nPlotly.newPlot(targetEle, data, layout, config);\n}\n};\n</code></pre>"},{"location":"Javascript/plotly/#rezise-charts-to-fit-container-in-bootstrap-v5-tabs","title":"Rezise chart(s) to fit container in bootstrap (v5) tabs","text":"<pre><code>const initTabsEvts = () =&gt; {\nconst tabEl = document.querySelectorAll('a[data-bs-toggle=\"tab\"]');\ntabEl.forEach((ele) =&gt; {\nele.addEventListener(\"shown.bs.tab\", function (event) {\nconst charts = document.querySelectorAll(\n\".tab-pane.active .plotly-charts\"\n);\ncharts.forEach((chart) =&gt; {\nPlotly.relayout(chart, { autosize: true });\n});\n});\n});\n};\n</code></pre>"},{"location":"Javascript/tabulator/","title":"Tabulator","text":"<p>Tabulator</p> <pre><code>const initTable = (dataUrl) =&gt; {\nconst table = new Tabulator(\"#table\", {\najaxURL: dataUrl,\nlayout: \"fitColumns\",\nindex: \"id\",\nmaxHeight: \"50vh\",\nresizableColumns: false,\nplaceholder: \"No Data Available\",\ninitialSort: [{ column: \"timestamp\", dir: \"desc\" }],\ncolumns: [\n{\nfield: \"id\",\ntitle: \"ID\",\nsorter: \"string\",\nvisible: false,\n},\n{\nfield: \"timestamp\",\ntitle: \"Timestamp\",\nsorter: \"datetime\",\nformatter: \"datetime\",\nformatterParams: {\ninputFormat: \"yyyy-MM-dd HH:mm:ss\",\noutputFormat: \"yyyy-MM-dd HH:mm:ss\",\n},\nheaderFilter: false,\n},\n{\nfield: \"date_val\",\ntitle: \"Date value\",\nsorter: \"date\",\nsorterParams: {\nalignEmptyValues: \"bottom\",\nformat: \"yyyy-MM-dd\",\n},\nformatter: \"date\",\nformatterParams: {\ninputFormat: \"yyyy-MM-dd\",\noutputFormat: \"yyyy-MM-dd\",\n},\nheaderFilter: true,\n},\n{\nfield: \"boolean_val\",\ntitle: \"Boolean value\",\nsorter: \"boolean\",\nformatter: \"tickCross\",\nheaderFilter: \"tickCross\",\nheaderFilterParams: { tristate: true },\nhozAlign: \"center\",\nformatterParams: {\nallowEmpty: true,\nallowTruthy: true,\ntickElement: \"Yes\",\ncrossElement: false,\n},\n},\n{\nfield: \"icon\",\ntitle: \"Icon\",\nheaderFilter: false,\nhozAlign: \"center\",\nformatter: iconLink,\n},\n],\n});\n};\nconst iconLink = (cell, formatterParams) =&gt; {\nconst val = cell.getValue();\nif (!!val) {\nreturn `&lt;i class=\"fe fe-link pointer\" data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" title=\"${val}\"&gt;&lt;/i&gt;`;\n}\nreturn \"\";\n};\nconst refreshTableDate = (dataUrl) =&gt; {\ntable.replaceData(dataUrl);\n};\n</code></pre>"},{"location":"Python/pandas/","title":"Pandas","text":""},{"location":"Python/pandas/#flatten-entityattributevalue-modelstables-using-pandas-dataframe","title":"Flatten entity\u2013attribute\u2013value models/tables using Pandas dataframe","text":"<p>users (entity/object table)</p> id name email 1 John Smith john.smith@example.com 2 Jane Smith jane.smith@example.com <p>page_config (attribute table)</p> id name 1 font-size 2 color 3 background-color <p>users_page_config (EAV table)</p> id user_id page_config_id value 1 1 1 12px 2 1 2 #3e3e3e 3 1 3 blue 4 2 1 16px 5 2 2 #000000 6 2 3 green <pre><code>import numpy as np\nimport pandas as pd\nclass EavFlattener:\ndef flatten_eav_table(self, data):\nresult_df = pd.DataFrame(data)\nresult_df = result_df.pivot(index=\"user_id\", columns='attribute')['attribute_value']\nresult_df = result_df.reset_index()\nresult_df = result_df.drop(np.nan, axis=1)\nreturn result_df\ndata = [\n{'user_id': 1, 'attribute': 'font-size', 'attribute_value': '12px'},\n{'user_id': 1, 'attribute': 'color', 'attribute_value': '#3e3e3e'},\n{'user_id': 1, 'attribute': 'background-color', 'attribute_value': 'blue'},\n{'user_id': 2, 'attribute': 'font-size', 'attribute_value': '16px'},\n{'user_id': 2, 'attribute': 'color', 'attribute_value': '#000000'},\n{'user_id': 2, 'attribute': 'background-color', 'attribute_value': 'green'}\n]\nEavFlattener().flatten_eav_table(data)\n# Flattened EAV data\n# [{'user_id': 1, 'background-color': 'blue', 'color': '#3e3e3e', 'font-size': '12px'},\n#  {'user_id': 2, 'background-color': 'green', 'color': '#000000', 'font-size': '16px'}]\n</code></pre>"},{"location":"Python/pyJWT/","title":"pyJWT","text":"<p>PyJWT</p>"},{"location":"Python/pyJWT/#validate-and-return-user-with-aws-cognito-access-token","title":"Validate and return user with AWS cognito access token","text":"<pre><code>import json\nfrom typing import Optional\nimport jwt\nimport requests\nfrom jwt.algorithms import RSAAlgorithm\nfrom path.to.user_model import User\nfrom path.to.config import config\nclass UserAuthService:\ndef get_current_user(self, access_token: str = None) -&gt; Optional[User]:\nvalid_token, current_user = self.validate_and_return_credentials(access_token)\nreturn current_user if (valid_token and current_user) else None\ndef get_public_access_keys(self) -&gt; str:\nauth = config.auth\nurl = f\"https://cognito-idp.{auth.region}.amazonaws.com/{auth.identity_pool_id}/.well-known/jwks.json\"\nresponse = requests.get(url)\nreturn response.text\ndef find_public_key(self, kid: str) -&gt; tuple[bool, Optional[dict]]:\npublic_keys = self.get_public_access_keys()\npublic_keys = json.loads(public_keys)\npublic_keys = {key[\"kid\"]: key for key in public_keys[\"keys\"]}\nmatched_key = public_keys.get(kid)\nreturn bool(matched_key), matched_key\ndef decode_access_token(self, public: dict, access_token: str, alg: str) -&gt; tuple[bool, dict]:\ntry:\npublic_key = RSAAlgorithm.from_jwk(json.dumps(public))\npayload = jwt.decode(\naccess_token,\npublic_key,\nalgorithms=[alg],\nverify=True,\noptions={\n\"verify_exp\": True,\n\"verify_nbf\": True,\n\"verify_iat\": False, # True\n\"verify_aud\": False, # True\n\"verify_iss\": True,\n},\naudience=config.auth.identity_pool_web_client,\nissuer=f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\",\n)\nexcept jwt.exceptions.DecodeError as exc:\nreturn False, {}\n# This should be check if token_use is ID token and \"verify_aud\" in jwt decode\nif payload[\"token_use\"] != \"access\":\nreturn False, {}\nif payload[\"client_id\"] != config.auth.identity_pool_web_client:\nreturn False, {}\nreturn True, payload\ndef validate_and_return_credentials(self, access_token: str) -&gt; tuple[bool, Optional[User]]:\nvalid_token, current_user = False, None\ntry:\nheaders = jwt.get_unverified_header(access_token)\nvalid_pub_key, user_public_key = self.find_public_key(headers.get(\"kid\"))\nif not user_public_key:\nreturn False, None\nvalid_signature, payload = self.decode_access_token(user_public_key, access_token, headers.get(\"alg\"))\nif valid_pub_key and valid_signature:\nvalid_token = True\nuser_sub_id = payload.get(\"sub\")\ncurrent_user = User.find_by(guid=user_sub_id) if user_sub_id else None\nexcept Exception as exc:\nreturn False, None\nreturn valid_token, current_user\n</code></pre>"},{"location":"Python/pyJWT/#with-alb-token-verification-alb-beast-vulnerability","title":"With ALB token verification (ALB Beast vulnerability)","text":"<pre><code>import json\nfrom typing import Optional, Tuple\nimport jwt\nimport requests\nfrom insig.logging.logger import Logger\nfrom jwt.algorithms import RSAAlgorithm\nfrom app.config import config\nfrom path.to.user_model import User\nclass UserAuthService:\ndef get_current_user(self, access_token: str = None, data_token: str = None):\nis_valid_alb_token = self.verify_alb_token(data_token)\nif not is_valid_alb_token:\nreturn None\nvalid_token, current_user = self.validate_access_token_and_return_credentials(\naccess_token\n)\nreturn current_user if (valid_token and current_user) else None\ndef get_public_access_keys(self, url: str) -&gt; Optional[str]:\ntry:\nresponse = requests.get(url)\nresponse.raise_for_status()\nreturn response.text\nexcept Exception as exc:\nlogger.warning(\"Auth Failed: Invalid public key %s\", str(exc))\nreturn None\ndef find_public_key(self, kid: str) -&gt; Tuple[bool, Optional[dict]]:\nauth = config.auth\nurl = f\"https://cognito-idp.{auth.region}.amazonaws.com/{auth.identity_pool_id}/.well-known/jwks.json\"\npublic_keys = self.get_public_access_keys(url)\nif not public_keys:\nlogger.warning(\"Auth Failed: Invalid cognito public key\")\nreturn False, {}\npublic_keys = json.loads(public_keys)\npublic_keys = {key[\"kid\"]: key for key in public_keys[\"keys\"]}\nmatched_key = public_keys.get(kid)\nreturn bool(matched_key), matched_key\ndef decode_access_token(self, public: dict, access_token: str, alg: str) -&gt; tuple[bool, dict]:\ntry:\npublic_key = RSAAlgorithm.from_jwk(json.dumps(public))\npayload = jwt.decode(\naccess_token,\npublic_key,\nalgorithms=[alg],\nverify=True,\noptions={\n\"verify_exp\": True,\n\"verify_nbf\": True,\n\"verify_iat\": False,\n\"verify_aud\": False,\n\"verify_iss\": True,\n},\naudience=config.auth.identity_pool_web_client,\nissuer=f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\",\n)\nexcept jwt.exceptions.DecodeError as exc:\nlogger.warning(\"Auth Failed - JWT DecodeError: %s\", str(exc))\nreturn False, {}\n# This should be check if token_use is ID token and \"verify_aud\" in jwt decode\nif payload[\"token_use\"] != \"access\":\nreturn False, {}\nif payload[\"client_id\"] != config.auth.identity_pool_web_client:\nreturn False, {}\nreturn True, payload\ndef verify_alb_token(self, data_token: str) -&gt; bool:\nif not data_token:\nreturn False\nheaders = jwt.get_unverified_header(data_token)\nauth = config.auth\nexpected_signer = (\nf\"arn:aws:elasticloadbalancing:{auth.region}:{auth.alb_id}:\"\nf\"loadbalancer/app/{auth.load_balancer_name}/{auth.load_balancer_id}\"\n)\nif not headers.get(\"signer\") == expected_signer:\nlogger.warning(\"Auth Failed: Invalid ALB signer\")\nreturn False\nurl = f'https://public-keys.auth.elb.{auth.region}.amazonaws.com/{headers.get(\"kid\")}'\npublic_key = self.get_public_access_keys(url)\nif not public_key:\nlogger.warning(\"Auth Failed: Invalid ALB public key\")\nreturn False\nexpected_issuer = f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\"\npayload = jwt.decode(\ndata_token,\npublic_key,\nalgorithms=[headers[\"alg\"]],\nverify=True,\noptions={\n\"verify_exp\": True,\n\"verify_nbf\": True,\n\"verify_iat\": False,\n\"verify_aud\": False,\n\"verify_iss\": True,\n},\naudience=config.auth.identity_pool_web_client,\nissuer=expected_issuer,\n)\nreturn payload.get(\"iss\") == expected_issuer\ndef validate_access_token_and_return_credentials(self, access_token: str) -&gt; Tuple[bool, Optional[str], bool]:\nvalid_token, user_sub_id, current_user = False, None, None\ntry:\nheaders = jwt.get_unverified_header(access_token)\nvalid_pub_key, user_public_key = self.find_public_key(headers.get(\"kid\"))\nif not user_public_key:\nlogger.warning(\"Auth Failed: Key not found\")\nreturn False, None, None\nvalid_signature, payload = self.decode_access_token(user_public_key, access_token, headers.get(\"alg\"))\nif valid_pub_key and valid_signature:\nvalid_token = True\nuser_sub_id = payload.get(\"sub\")\ncurrent_user = User.find_by(guid=user_sub_id) if user_sub_id else None\nexcept Exception as exc:\nlogger.warning(\"Auth Failed: %s\", str(exc))\nreturn valid_token, current_user\n</code></pre>"},{"location":"Python/python%20notes/","title":"Python notes","text":""},{"location":"Python/python%20notes/#logging-class","title":"Logging class","text":"<pre><code>import logging\nclass Logger:\ndef __init__(self):\nself._logger = None\ndef setup_logger(self):\nlogger = logging.getLogger(\"root\")\nif logger.handlers:\nreturn logger\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(module)s %(funcName)s - %(message)s\")\nhandler = logging.StreamHandler()\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nreturn logger\ndef get_logger(self):\nif self._logger is None:\nself._logger = self.setup_logger()\nreturn self._logger\n</code></pre> <p>To use:</p> <pre><code>from path.to.logger import Logger\nlogger = Logger().get_logger()\n</code></pre>"},{"location":"Python/python%20notes/#creating-decorator","title":"Creating decorator","text":"<pre><code>def logged_in_user(function) -&gt; Any:\n@wraps(function)\ndef get_logged_in_user(*args, **kwargs):\ncurrent_user = get_valid_current_user()\nif not current_user:\nreturn reject_access()\nreturn function(*args, **kwargs, current_user=current_user)\nreturn get_logged_in_user\n@app.post('/')\n@logged_in_user\ndef app_root_page(current_user):\n...\n</code></pre>"},{"location":"Python/python%20notes/#creating-batch-from-list","title":"creating batch from list","text":"<pre><code>def create_batch(self, obj_ls: list[\"ObjCls\"], batch_size: int):\nfor i in range(0, len(obj_ls), batch_size):\nyield obj_ls[i : i + batch_size]\n</code></pre>"},{"location":"Python/python%20notes/#list-filtering","title":"List filtering","text":"<p>Supposing we need to filter out the results from the below list of dictionaries</p> <pre><code>exam_results = [\n{\"name\": \"John\", \"score\": 70, \"subject\": \"German\"},\n{\"name\": \"Jane\", \"score\": 80, \"subject\": \"Maths\"},\n{\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"},\n{\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"},\n{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"},\n{\"name\": \"Theo\", \"score\": 40, \"subject\": \"Chemistry\"},\n]\n</code></pre> <p>To retrieve exam results where <code>score</code> is higher than 65:</p> <pre><code>filtered_result = list(filter(lambda x: x[\"score\"] &gt; 65, exam_results))\nprint(filtered_result)\n# [{\"name\": \"John\", \"score\": 70, \"subject\": \"German\"}, {\"name\": \"Jane\", \"score\": 80, \"subject\": \"Maths\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre> <p>To retrieve exam results where <code>score</code> is equal or higher than 50 and subject is either <code>Chemistry</code>, <code>French</code>, <code>Graphics</code>:</p> <pre><code>filtered_subject_results = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] in ([\"Chemistry\", \"French\", \"Graphics\"]), exam_results))\nprint(filtered_subject_results)\n# [{\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}, {\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}]\n</code></pre> <p>...and with <code>subject</code> in specific order using for-loop</p> <pre><code>filtered_subject_results = []\nfor subject_name in [\"Chemistry\", \"French\", \"Graphics\"]:\nresult = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] == subject_name, exam_results))\nfiltered_subject_results = filtered_subject_results + result\nprint(filtered_subject_results)\n# [{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}, {\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre> <p>...or with <code>sorted</code></p> <pre><code>filtered_subject_results = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] in ([\"Chemistry\", \"French\", \"Graphics\"]), exam_results))\nfiltered_subject_results = sorted(filtered_subject_results, key=lambda x: (x[\"subject\"]))\nprint(filtered_subject_results)\n# [{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}, {\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre>"},{"location":"Python/python%20notes/#sort-groupby-itertools","title":"sort &amp; groupby (itertools)","text":"<p>Assuming we have a list of dictionaries of students needed to be grouped by year-class:</p> <pre><code>students_ls = [\n{\"name\": \"John\", \"email\": \"john@random_school.edu.uk\", \"year\": 1, \"class\": \"A\"},\n{\"name\": \"Jane\", \"email\": \"jane@random_school.edu.uk\", \"year\": 1, \"class\": \"B\"},\n{\"name\": \"Mary\", \"email\": \"mary@random_school.edu.uk\", \"year\": 2, \"class\": \"C\"},\n{\"name\": \"Alex\", \"email\": \"alex@random_school.edu.uk\", \"year\": 2, \"class\": \"A\"},\n{\"name\": \"Sam\", \"email\": \"sam@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n{\"name\": \"Hannah\", \"email\": \"hannah@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n{\"name\": \"Kim\", \"email\": \"kim@random_school.edu.uk\", \"year\": 3, \"class\": \"B\"},\n{\"name\": \"Ted\", \"email\": \"ted@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n]\n</code></pre> <p>First, we have to sorted the list by year and class as the documentation states that the list need to be sorted before applying <code>groupby</code></p> <pre><code>sorted_students_ls = sorted(students_ls, key=lambda x: (x[\"year\"], x[\"class\"]))\nprint(sorted_students_ls)\n# [\n#   {'class': 'A', 'email': 'john@random_school.edu.uk', 'name': 'John', 'year': 1},\n#   {'class': 'B', 'email': 'jane@random_school.edu.uk', 'name': 'Jane', 'year': 1},\n#   {'class': 'A', 'email': 'alex@random_school.edu.uk', 'name': 'Alex', 'year': 2},\n#   {'class': 'C', 'email': 'mary@random_school.edu.uk', 'name': 'Mary', 'year': 2},\n#   {'class': 'A', 'email': 'sam@random_school.edu.uk', 'name': 'Sam', 'year': 3},\n#   {'class': 'A', 'email': 'hannah@random_school.edu.uk', 'name': 'Hannah', 'year': 3},\n#   {'class': 'A', 'email': 'ted@random_school.edu.uk', 'name': 'Ted', 'year': 3},\n#   {'class': 'B', 'email': 'kim@random_school.edu.uk', 'name': 'Kim', 'year': 3}\n# ]\n</code></pre> <p>Then apply <code>groupby</code>:</p> <pre><code>grouped_data_single_line = {f\"{k[0]}-{k[1]}\": list(students) for k, students in itertools.groupby(sorted_students_ls, key=lambda x: (x[\"year\"], x[\"class\"]))}\nprint(grouped_data_single_line)\n# {\n#   \"1-A\": [{'class': 'A', 'email': 'john@random_school.edu.uk', 'name': 'John', 'year': 1}],\n#   \"1-B\": [{'class': 'B', 'email': 'jane@random_school.edu.uk', 'name': 'Jane', 'year': 1}],\n#   \"2-A\": [{'class': 'A', 'email': 'alex@random_school.edu.uk', 'name': 'Alex', 'year': 2}],\n#   \"2-C\": [{'class': 'C', 'email': 'mary@random_school.edu.uk', 'name': 'Mary', 'year': 2}],\n#   \"3-A\": [{'class': 'A', 'email': 'sam@random_school.edu.uk', 'name': 'Sam', 'year': 3},\n#           {'class': 'A', 'email': 'hannah@random_school.edu.uk', 'name': 'Hannah', 'year': 3},\n#           {'class': 'A', 'email': 'ted@random_school.edu.uk', 'name': 'Ted', 'year': 3}],\n#   \"3-B\": [{'class': 'B', 'email': 'kim@random_school.edu.uk', 'name': 'Kim', 'year': 3}]\n# }\n</code></pre> <p>or below if require data formatting</p> <pre><code>grouped_data = {}\nfor k, students in itertools.groupby(sorted_students_ls, key=lambda x: (x[\"year\"], x[\"class\"])):\ndata_key = f\"{k[0]}-{k[1]}\"\ngrouped_data[data_key] = [{\"name\": student[\"name\"], \"email\": student[\"email\"]} for student in list(students)]\nprint(grouped_data_single_line)\n# {\n#     '1-A': [{'email': 'john@random_school.edu.uk', 'name': 'John'}],\n#     '1-B': [{'email': 'jane@random_school.edu.uk', 'name': 'Jane'}],\n#     '2-A': [{'email': 'alex@random_school.edu.uk', 'name': 'Alex'}],\n#     '2-C': [{'email': 'mary@random_school.edu.uk', 'name': 'Mary'}],\n#     '3-A': [{'email': 'sam@random_school.edu.uk', 'name': 'Sam'},\n#             {'email': 'hannah@random_school.edu.uk', 'name': 'Hannah'},\n#             {'email': 'ted@random_school.edu.uk', 'name': 'Ted'}],\n#     '3-B': [{'email': 'kim@random_school.edu.uk', 'name': 'Kim'}]\n# }\n</code></pre>"},{"location":"Python/python%20notes/#dates-related","title":"Dates related","text":"<p> Get last day of week from a given date </p> <pre><code>from datetime import date, datetime, timedelta\ndef get_week_end(date_input: date):\n# use 6 for week ending on Sunday\nreturn date_input + timedelta(days=5 - date_input.weekday())\n</code></pre> <p> Get last day of month from a given date </p> <pre><code>import bisect\nimport calendar\nfrom datetime import date, datetime, timedelta\ndef get_month_end(date_input: date):\nlast_day_of_month = calendar.monthrange(date_input.year, date_input.month)[1]\nreturn date(date_input.year, date_input.month, last_day_of_month)\n</code></pre> <p> Get start &amp; end of quarter from a given date </p> <pre><code>import bisect\nimport calendar\nfrom datetime import date, datetime, timedelta\ndef get_quarter_end(date_input: date):\nquarter_ends = [date(date_input.year, month, 1) + timedelta(days=-1) for month in (4, 7, 10)]\nquarter_ends.append(date(date_input.year + 1, 1, 1) + timedelta(days=-1))\nidx = bisect.bisect_left(quarter_ends, date_input)\nreturn quarter_ends[idx]\ndef get_quarter_start(date_input: date):\nquarter_start = [date(date_input.year, month, 1) for month in (1, 4, 7, 10)]\nidx = bisect.bisect(quarter_start, date_input)\nreturn quarter_start[idx - 1]\n</code></pre>"},{"location":"Python/python%20notes/#user-authorisationpermission","title":"User authorisation/permission","text":"<p>An tiny user authorisation/permission python/flask class inspired by Ruby's authorization library Pundit</p> <pre><code>import inspect\nimport re\nfrom flask import g, request\nfrom path.to.user.model import User\n@staticmethod\ndef to_snake_case(obj_str):\nreg_match = re.compile(\"((?&lt;=[a-z0-9])[A-Z]|(?!^)(?&lt;!_)[A-Z](?=[a-z]))\")\nreturn reg_match.sub(r\"_\\1\", obj_str).lower()\nclass UserAuth:\ndef __init__(self):\nself._current_user = None\n@property\ndef current_user(self):\nif self._current_user is None:\n# can replace with g.current_user or method to get current user\nself._current_user = User.get(g.user_id)\nreturn self._current_user\n@classmethod\ndef get_policy_cls(cls, model_cls):\nmodel_cls_name = model_cls.__name__\nmodel_name_snake_case = to_snake_case(model_cls_name)\npolicy_path = f\"path.to.policies.{model_name_snake_case}_policy\"\npolicy_module = __import__(policy_path, fromlist=[f\"{model_cls_name}Policy\"])\npolicy_cls = getattr(policy_module, f\"{model_cls_name}Policy\")\nreturn policy_cls\n@classmethod\ndef authorised_action(cls, model_obj, action=None, *args, **kwargs):\nmodel_cls = cls.get_model_class(model_obj)\nmodel_policy = cls.get_policy_cls(model_cls)\naction = action or request.method.lower()\nreturn getattr(model_policy(self.current_user, model_obj), action)(*args, **kwargs)\n@classmethod\ndef authorised_scope(cls, model_obj, *args, **kwargs):\nmodel_cls = cls.get_model_class(model_obj)\nmodel_policy = cls.get_policy_cls(model_cls)\nreturn getattr(model_policy(self.current_user, model_cls), \"scope\")(*args, **kwargs)\n@classmethod\ndef get_model_class(cls, model_obj):\nif inspect.isclass(model_obj):\nreturn model_obj\nreturn model_obj.__class__\n</code></pre>"},{"location":"Python/python%20notes/#how-it-works","title":"How it works","text":"<p>Assuming we have a <code>User</code> class similar to the example below (with SQLAlchemy):</p> <pre><code># Path to SQLalchemy/Database config\nfrom app.setting.database import db\nclass UserRole(enum.Enum):\nREGISTER = 1\nADMIN = 2\nclass User(db.Model):\n# for dataclass, can do this instead of `__init__`\n# id: uuid.UUID\n# email: str\n# first_name: str\n# role: UserRole = UserRole.REGISTER\ndef __init__(self, email: str, name: str, role: int = UserRole.REGISTER) -&gt; None:\nself.email = email\nself.name = name\nself.role = role\n@classmethod\ndef create(cls, email: str, name: str, role: int = UserRole.REGISTER) -&gt; \"User\":\nuser = cls(email=email, name=name, role=role)\ndb.session.add(user)\ndb.session.commit()\nreturn user\n@classmethod\ndef get(cls, id: str) -&gt; \"User\":\nreturn cls.query.get(id)\n@classmethod\ndef all(cls) -&gt; list[\"User\"]:\nreturn cls.query.all()\ndef update(self, name: str, email:str) -&gt; None:\nself.name = name\nself.email = email\ndb.session.commit()\n@property\ndef is_admin(self) -&gt; bool:\nreturn self.role == UserRole.ADMIN\n</code></pre> <p>And <code>UserPolicy</code> - assuming only admin users can create, read, update and view all users, while registered users can only view their only user accounts:</p> <pre><code>class UserPolicy:\ndef __init__(self, user: User, user_obj: User) -&gt; None:\nself.user = user\nself.user_obj = user_obj\ndef create(self) -&gt; bool:\nreturn self.user.is_admin:\ndef update(self) -&gt; bool:\nreturn self.user.is_admin:\ndef get(self) -&gt; bool:\nreturn (self.user.id == self.user.id) or self.user.is_admin\ndef scope(self) -&gt; list[User]:\nif self.user.is_admin:\nreturn self.user_obj.all()\nreturn [self.user_obj.get(self.user.id)]\n</code></pre> <p>(Optional) And a custom exception class when current user not authorised to perform particular action(s) defined in the policy:</p> <pre><code>class PermissionRequired(BaseError):\ndef __init__(self):\nsuper().__init__(403, \"Permission denied\")\n</code></pre> <p>To check if the current user is authorised to perform a particular <code>User</code> action - in this example update, in a routing/view:</p> <pre><code>from path.to.user_auth import UserAuth\nfrom path.to.user.model import User\n@app.post('/users/&lt;user_id&gt;')\ndef update(user_id):\nuser = User.get(user_id)\nif UserAuth.authorize(user, \"update\"):\nreturn user\nreturn render_template(\"403.html\"), 403\n@app.post('/users')\ndef all():\nuser_records = UserAuth.authorised_scope(User)\nreturn jsonify(user_records)\n</code></pre> <p>or in a service class:</p> <pre><code>from path.to.user_auth import UserAuth\nfrom path.to.user.model import User\nclass UsersService:\ndef update(self, user_id: str, name: str, email: str) -&gt; dict[str, str]:\nuser = User.get(user_id)\nif not UserAuth.authorised_action(user, \"update\"):\nraise PermissionRequired()\nuser.update(name=name, email=email)\nreturn {\"message\": \"User successfully update\"}\n</code></pre> <p>The <code>UserAuth.authorize</code> takes two parameters:</p> <ul> <li>the model object (can be an instance or a class) you want to authorise the current user on</li> <li>the action or class method to authorise - in this case <code>update</code>. By default, if nothing passes as the parameter it will use the <code>request.method</code>, e.g. <code>post</code> in the example, and will require to define a <code>post</code> method in the policy:</li> </ul> <pre><code>class UserPolicy:\n...\ndef post(self) -&gt; bool:\nreturn self.user.is_admin:\n...\n</code></pre>"},{"location":"Python/redis/","title":"Redis","text":""},{"location":"Python/redis/#cache-helper","title":"Cache helper","text":"<pre><code>from typing import TYPE_CHECKING, Any, Optional\nimport redis\nfrom app.config.config import config\nif TYPE_CHECKING:\nfrom datetime import datetime\nclass RedisCache:\ndef __init__(self, decode_responses=True) -&gt; None:\nhost = config.redis_db.host\nport = config.redis_db.port\nself.redis_client = redis.StrictRedis(host=host, port=port, decode_responses=decode_responses)\ndef set(self, key: str, value: Any) -&gt; Optional[bool]:\nself.redis_client.set(key, value)\ndef get(self, key: str) -&gt; Optional[bytes]:\nreturn self.redis_client.get(key)\ndef set_key_expire_at(self, key: str, expire_at: \"datetime\") -&gt; None:\nif self.redis_client.exists(key):\nself.redis_client.expireat(key, expire_at)\ndef get_keys_by_pattern(self, keys_pattern: str) -&gt; list:\nreturn [cache_key for cache_key in self.redis_client.scan_iter(keys_pattern)]\ndef delete_keys_by_pattern(self, keys_pattern: str) -&gt; list:\nfor cache_key in self.redis_client.scan_iter(keys_pattern):\nself.redis_client.delete(cache_key)\ndef flushall(self) -&gt; None:\nself.redis_client.flushall()\ndef exists(self, key) -&gt; int:\nreturn self.redis_client.exists(key)\ndef store_and_set_expire(self, cache_key: str, data: str, expire_at: \"datetime\") -&gt; None:\nself.set(cache_key, data)\nself.set_key_expire_at(cache_key, expire_at)\n</code></pre>"},{"location":"Python/redis/#events-streaming-with-redis-stream","title":"Events streaming with Redis Stream","text":"<p>Redis documentation on Stream Use of consumer groups Redis-py</p>"},{"location":"Python/redis/#publisher","title":"Publisher","text":"<pre><code>import json\nfrom concurrent import futures\nfrom datetime import datetime\nfrom typing import Any\nimport redis\nfrom logger import Logger\nclass EventsPublisher:\ndef __init__(self, event_stream_name: str) -&gt; None:\nself.logger = Logger().get_logger()\nself.event_stream_name = event_stream_name\nself._publisher = None\n@properity\ndef publisher(self):\nif self._publisher is None:\nself._publisher = redis.Redis(\nhost=config.event_publisher.host,\nport=config.event_publisher.port,\ndb=config.event_publisher.database,\n)\nreturn self._publisher\ndef batch_add(self, payloads: list, id_key=\"obj_id\") -&gt; tuple[list[str], list[str]]:\nsuccessful_entries_ids = []\nfailed_obj_ids = []\nwith futures.ThreadPoolExecutor() as executor:\ntasks = [executor.submit(self.add, payload, False, id_key) for payload in payloads]\nfor task in futures.as_completed(tasks):\ntry:\nentry_id, obj_id = task.result()\nif entry_id is not None:\nsuccessful_entries_ids.append(entry_id)\nelse:\nfailed_obj_ids.append(obj_id)\nexcept Exception:\nself.logger.exception(\"Error pushing data to stream\")\nreturn successful_entries_ids, failed_obj_ids\ndef add(self, payload: Any, raise_error=True, id_key=\"id\") -&gt; tuple[str, str]:\nobj_id = payload.get(id_key)\nobj_type = payload.get(\"obj_type\")\naction = payload.get(\"action\") # create / update etc\ntry:\njson_payload = json.dumps(payload)\nself.logger.info(\"Pushing %s data (%s) with ID %s to stream\", obj_type, action, obj_id)\nentry_id = self.publisher.xadd(self.event_stream_name, {\"payload\": json_payload})\nentry_id = entry_id.decode(\"utf-8\")\nself.logger.info(\n\"%s data (%s) with ID %s pushed - entry_id %s\", obj_type, action, obj_id, entry_id\n)\nreturn entry_id, obj_id\nexcept (\nTypeError,\nredis.exceptions.ConnectionError,\nredis.exceptions.TimeoutError,\nredis.exceptions.RedisError,\n) as e:\nmsg = f\"Failed to push {obj_type} data ({action}) with ID {obj_id} to stream: {str(e)}\"\nself.logger.info(msg)\nif raise_error:\nraise e\nreturn None, obj_id\ndef delete(self, msg_id: str) -&gt; dict[str, int]:\nstatus = self.publisher.xdel(self.event_stream_name, msg_id)\nreturn {msg_id: status}\ndef get_list(self, msg_count=10):\nevent_msgs = self.publisher.xread({self.event_stream_name: \"0-0\"}, count=msg_count)\ndata = {}\nif not event_msgs:\nreturn data\nfor message in event_msgs[0][1]:\nmessage_id = message[0].decode(\"utf-8\")\ntry:\nmessage_body = message[1]\npayload = message_body.get(b\"payload\")\npayload = payload.decode(\"utf-8\") if payload else {}\nexcept Exception:\npayload = {}\ndata[message_id] = payload\nreturn data\n</code></pre>"},{"location":"Python/redis/#consumer-one-consumer-no-additional-consumer-groups","title":"Consumer (One consumer - no additional consumer groups)","text":"<pre><code>import asyncio\nimport json\nfrom typing import Any\nfrom redis import asyncio as aioredis\nfrom path.to.config import config\nclass EventsConsumer:\ndef __init__(self, app) -&gt; None:\napp.app_context().push()\nself.logger = logger\nasync def handle_event(self, payload: dict) -&gt; None:\n# process the event message body\nawait process_event_message(payload)\nasync def main(self) -&gt; Any:\nconsumer_config = config.event_consumer\nconsumer_host = consumer_config.stream_host\nconsumer_port = consumer_config.stream_port\nconsumer_database = consumer_config.stream_database\nconsumer_stream = consumer_config.stream\nredis = await aioredis.from_url(\nf\"redis://{consumer_host}:{consumer_port}\",\ndb=consumer_database,\nencoding=\"utf8\",\ndecode_responses=True,\n)\nmessage_id = \"0-0\"\nwhile True:\nevents = await redis.xread({consumer_stream: message_id})\nif not events:\nawait asyncio.sleep(1)\nfor stream, message in events:\nmessage_id = message[0][0]\npayload = message[0][1][\"payload\"]\npayload = json.loads(payload)\nawait self.handle_event(payload)\nawait redis.xdel(stream, message_id)\n</code></pre> <p>To start consumer as thread:</p> <pre><code>def start_event_consumer():\nasyncio.run(EventsConsumer(app).main())\nworker = threading.Thread(target=start_event_consumer, args=[])\nworker.start()\n</code></pre>"},{"location":"Python/redis/#consumer-groups","title":"Consumer groups","text":"<p>Create consumer group(s) XGROUP CREATE:</p> <pre><code>redis_client.xgroup_create(name=stream_name, groupname=gname, id=0)\n</code></pre> <p>Read with consumer group(s) XREADGROUP:</p> <pre><code>redis_client.xreadgroup(groupname=group_1, consumername='consumer_a', streams={stream_key:'&gt;'})\n</code></pre> <p>Acknowledge messages XACK:</p> <pre><code>redis_client.xack(stream_name, groupname, message_id)\n</code></pre>"},{"location":"Python/sqlalchemy/","title":"Sqlalchemy","text":""},{"location":"Python/sqlalchemy/#intenum","title":"IntEnum","text":"<p>Storing the enum integer value to the database</p> <pre><code>from sqlalchemy import types\nclass IntEnum(types.TypeDecorator):\nimpl = Integer\ndef __init__(self, enumtype, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself._enumtype = enumtype\ndef process_bind_param(self, value, dialect):\nreturn value.value\ndef process_result_value(self, value, dialect):\nreturn self._enumtype(value)\n</code></pre> <pre><code>class Role(enum.Enum):\nREGISTERED = 1\nMODERATOR = 2\nADMIN = 3\nclass User(db.Model):\n__tablename__ = \"users\"\nid: uuid.UUID = db.Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\nname: str = db.Column(db.String(50), nullable=False)\nemail: str = db.Column(db.String(50), nullable=False)\nrole: int = db.Column(IntEnum(Role), nullable=False, default=1)\n</code></pre>"},{"location":"Python/sqlalchemy/#query-mixin","title":"Query mixin","text":"<pre><code>class QueryMixin:\n@classmethod\ndef get(cls, id):\nreturn cls.query.get(id)\n@classmethod\ndef _filters(cls, kwargs):\nreturn [getattr(cls, attr) == kwargs[attr] for attr in kwargs]\n@classmethod\ndef find_by(cls, **kwargs):\nfilters = cls._filters(kwargs)\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().first()\n@classmethod\ndef find_all(cls, **kwargs):\nfilters = cls._filters(kwargs)\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().all()\n@classmethod\ndef find_all_in(cls, **kwargs):\nfilters = [getattr(cls, attr).in_(kwargs[attr]) for attr in kwargs]\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().all()\n@classmethod\ndef find_all_not_in(cls, **kwargs):\nfilters = [getattr(cls, attr).not_in(kwargs[attr]) for attr in kwargs]\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().all()\n@classmethod\ndef delete_if_exists(cls, **kwargs):\nfilters = cls._filters(kwargs)\ncls.query.where(*filters).delete()\ndb.session.commit()\ndef delete(self):\ndb.session.delete(self)\ndb.session.commit()\ndef to_dict(self):\nreturn {\ncolumn.name: getattr(self, column.name)\nif not isinstance(getattr(self, column.name), (datetime, date))\nelse getattr(self, column.name).isoformat()\nfor column in self.__table__.columns\n}\n</code></pre>"},{"location":"Python/sqlalchemy/#insertreturning","title":"Insert...returning","text":"<pre><code>    data = [\n{\"name\": \"Sally\", \"email\": \"sally@user.email\"},\n{\"name\": \"Jon\", \"email\": \"jon@user.email\"},\n{\"name\": \"Ken\", \"email\": \"ken@user.email\"},\n{\"name\": \"Jess\", \"email\": \"jess@user.email\"}\n]\nemployees = db.session.execute(insert(Employee).returning(Employee), data).all()\n</code></pre>"},{"location":"Python/sqlalchemy/#updatereturning","title":"Update...returning","text":"<pre><code>    employee_id = \"ec38f27b-79a2-4739-b96a-6bc2babcc2c9\"\nupdate_params = {\"name\": \"Ken\", \"email\": \"ken@another.email\"}\nupdate_stmt = update(Employee).where(Employee.id == employee_id).values(update_params).returning(Employee)\nemployees = db.session.execute(update_stmt).first()\n</code></pre>"},{"location":"Python/sqlalchemy/#update-multiple-records-where-the-where-conditons-is-different-for-each-record","title":"Update multiple records where the <code>WHERE</code> conditons is different for each record","text":"<pre><code>    data = [\n{\"employee_id\": \"443bd75a-3baa-4b17-a391-af00af9e3325\", \"name\": \"Sally\", \"email\": \"sally@user.email\", \"new_department\": \"Marketing\"},\n{\"employee_id\": \"52ca61aa-399d-4a0c-9482-6e0ec3ab4891\", \"name\": \"Jon\", \"email\": \"jon@user.email\", \"new_department\": \"IT\"},\n{\"employee_id\": \"ec38f27b-79a2-4739-b96a-6bc2babcc2c9\", \"name\": \"Ken\", \"email\": \"ken@user.email\", \"new_department\": \"Finance\"},\n{\"employee_id\": \"0d28cbbd-f073-4420-9f73-3dfc93c7696e\", \"name\": \"Jess\", \"email\": \"jess@user.email\", \"new_department\": \"Marketing\"},\n]\nupdate_stmt = (\nupdate(Employee)\n.where(Employee.id == bindparam(\"employee_id\"))\n.values(\n{\"department_id\": select(Department.id).where(Department.name == bindparam(\"new_department\")).scalar_subquery()}\n)\n)\ndb.session.execute(update_stmt, data)\n</code></pre>"},{"location":"Python/sqlalchemy/#nested-transaction","title":"Nested transaction","text":"<pre><code>    db.session.begin_nested()\n</code></pre>"},{"location":"Python/testing/","title":"Testing","text":""},{"location":"Python/testing/#pytest","title":"Pytest","text":""},{"location":"Python/testing/#testing-endpoints","title":"Testing endpoints","text":"<pre><code>class TestClsNamePage:\ndef setup_method(self):\nself.app = app.test_client()\nself.app_context = app.app_context()\nself.app_context.push()\n# Test get endpoint and assert content on page\ndef test_get_endpoint(self):\nresponse = self.app.get(\"/\")\nassert response.status_code == 200\npage_content = response.data.decode(\"utf-8\")\nassert \"Hello!\" in page_content\n# Test endpoint with headers\ndef test_endponint_with_headers(self):\nresponse = self.app.get(\"/\", headers={\"Auth-Token\": \"some-random-auth-token-values\"})\nassert response.status_code == 200\n# Test get endpoint and content on page after redirect\ndef test_get_endpoint(self):\nresponse = self.app.get(\"/redirect_to_another_page\", follow_redirects=True)\nassert response.status_code == 200\npage_content = response.data.decode(\"utf-8\")\nassert \"Redirected\" in page_content\n# Test get endpoint with JSON response\ndef test_get_endpoint(self):\n# assuming endpoint return {\"message\": \"Hello!\"}\nresponse = self.app.get(\"/json_response\")\nassert response.status_code == 200\ndata = json.loads(response.data)\nassert data[\"message\"] == \"Hello!\"\n# Test post endpoint - HTML form submission\ndef test_post_form_submit(self):\nresponse = self.app.post(\"/users\", data={\"name\": \"user\", \"email\": \"user@test.com\"})\nassert response.status_code == 200\n# Test post endpoint - HTML form submission with multi select\ndef test_post_multi_select_form_submit(self):\n# Remember to import ImmutableMultiDict: `from werkzeug.datastructures import ImmutableMultiDict`\nform = ImmutableMultiDict(\n[(\"user_id\", \"user_id_1\"), (\"user_id\", \"user_id_2\"), (\"user_id\", \"user_id_2\")]\n)\nresponse = self.app.post(\"/users/multi_select\", data=form)\nassert response.status_code == 200\n# Test post endpoint JSON params\ndef test_post_json_params(self):\nresponse = self.app.post(\"/users\", json={\"name\": \"user\", \"email\": \"user@test.com\"})\nassert response.status_code == 200\n</code></pre>"},{"location":"Python/testing/#parametrize-tests","title":"Parametrize tests","text":"<pre><code>import pytest\nclass TestClsName:\n@pytest.mark.parametrize(\"param_1, param_2, param3, expected_result\", [\n(test_1_param_1, test_1_param_2, test_1_param_3, test_1_expected_result),\n(test_2_param_1, test_2_param_2, test_2_param_3, test_2_expected_result),\n(test_3_param_1, test_3_param_2, test_3_param_3, test_3_expected_result),\n])\ndef test_some_test_with_parametrize(self, param_1, param_2, param3, expected_result):\nresult = method_to_test(param_1, param_2, param3)\nassert result == expected_result\n</code></pre>"},{"location":"Python/testing/#mockpatching","title":"Mock/patching","text":"<p> Mock method to return specific value </p> <pre><code>class TestClsName:\ndef test_method_with_patch(self):\nwith patch(\"path.to.class.ClassName.method_name\") as mocked_method:\nmocked_method.return_value = [1,2,3]\n</code></pre> <p> Mock method to raise exception </p> <pre><code>class TestClsName:\ndef test_method_with_mocked_exception(self):\nwith patch(\"path.to.class.ClassName.method_name\") as mocked_method:\nmocked_method.side_effect = Exception(\"error\")\n</code></pre> <p> Mock response with JSON response, status_code </p> <pre><code>class MockResponse:\ndef __init__(self, json_data, status_code):\nself.json_data = json_data\nself.status_code = status_code\ndef json(self):\nreturn self.json_data\nclass TestClsName:\ndef test_method_with_post_request(self):\nwith patch(\"app.model.ClsName.requests.post\") as mocked_request:\nmocked_request.return_value=MockResponse({\"message\": \"ta-da\", \"status\": \"success\"}, 200)\nresult = method_with_post_request()\n...\n</code></pre> <p> Mock response with raise_for_status </p> <pre><code>class TestClsName:\ndef test_method_with_post_request_raise_for_status(self):\nwith patch(\"app.model.ClsName.requests.post\") as mocked_request:\nmocked_status = Mock(status_code=500)\nmocked_status.raise_for_status = Mock(side_effect=requests.exceptions.RequestException(\"Error\"))\nmocked_request.return_value = mocked_status\nresult = method_with_post_request()\n...\n</code></pre>"},{"location":"Python/testing/#mock-aws-services-with-moto","title":"Mock AWS services with moto","text":"<p>For example, creating a S3 mock with moto</p> <pre><code>@pytest.fixture(scope=\"session\")\ndef aws_credentials():\n\"\"\"Mocked AWS Credentials for moto.\"\"\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\nos.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\nos.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\nos.environ[\"AWS_DEFAULT_REGION\"] = \"eu-west-1\"\n@pytest.fixture(autouse=False)\ndef s3_client(aws_credentials):\nwith mock_s3():\nconn = boto3.client(\"s3\", region_name=\"us-east-1\")\nyield conn\n</code></pre> <p>And in tests, use <code>s3_client</code> as fixture</p> <pre><code>@pytest.fixture\ndef s3_bucket(s3_client):\ns3_client.create_bucket(Bucket=bucket_name)\nyield\nclass TestS3Service:\ndef test_s3_bucket_list_objects(self, s3_client, s3_bucket):\nS3Service().list_objects()\n...\n</code></pre>"},{"location":"Python/testing/#monkeypatching-sqlalchemy-connection","title":"Monkeypatching SQLAlchemy connection","text":"<p> For SQLAlchemy 1.4 </p> <pre><code>@pytest.fixture\ndef app(request):\napp = _app\nwith app.app_context():\nyield app\n@pytest.fixture\ndef db(app, request, monkeypatch):\nconnection = _db.engine.connect()\ntransaction = connection.begin()\n# https://github.com/pallets/flask-sqlalchemy/pull/249#issuecomment-628303481\nmonkeypatch.setattr(_db, \"get_engine\", lambda *args, **kwargs: connection)\ntry:\nyield _db\nfinally:\n_db.session.remove()\ntransaction.rollback()\nconnection.close()\n</code></pre> <p>And use <code>db</code> as fixture</p> <pre><code>@pytest.mark.usefixtures(\"db\")\nclass TestClsName:\n...\n</code></pre> <pre><code>def test_method_name(self, db):\n...\n</code></pre> <p>When testing database rollback use <code>db.session.begin_nested()</code> to begin a \"nested\" transaction/savepoint</p> <pre><code>class TestClsName:\ndef test_method_with_db_rollback(self, db):\ncreate_test_objects()\ndb.session.begin_nested()\nresult = method_with_db_rollback()\nassert result == expected_result\n</code></pre>"},{"location":"Python/testing/#assert-exception-raised","title":"Assert exception raised","text":"<pre><code>class TestUser:\ndef test_user_init_failed_on_missing_required_values(self):\nwith pytest.raises(TypeError) as error:\nUser()\nassert str(error.value) == \"__init__() missing 2 required positional argument: 'name', 'email'\"\n</code></pre>"},{"location":"Python/testing/#assert-parameters-pass-to-methodmock-method","title":"Assert parameters pass to method/mock method","text":"<p>Passing the actual method to <code>side_effect</code> will call the actual method instead of \"mocked\"</p> <pre><code>    with patch.object(ClsName, \"method_name\", side_effect=ClsName().method_name) as mocked_method:\nmocked_method.assert_called_with(\nparam_1=expected_param_1,\nparam_2=expected_param_2,\nparam_3=expected_param_3,\n)\n</code></pre>"},{"location":"Python/testing/#mock-results-of-repeatedmultiple-calls-to-the-same-method","title":"Mock results of repeated/multiple calls to the same method","text":"<p>For example, calling a method in a loop:</p> <pre><code>def loop_me():\nfor i in range(3):\nresult = do_something()\nprint(result)\n</code></pre> <p>To mock the results of repeated <code>do_something</code> calls</p> <pre><code>    with patch(\"path.to.do_something\", side_effect=(4, 5, 6)):\nloop_me()\n</code></pre>"}]}